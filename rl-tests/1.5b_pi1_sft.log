W0814 00:57:42.610000 140663534395840 torch/distributed/run.py:779] 
W0814 00:57:42.610000 140663534395840 torch/distributed/run.py:779] *****************************************
W0814 00:57:42.610000 140663534395840 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0814 00:57:42.610000 140663534395840 torch/distributed/run.py:779] *****************************************
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
0       <|im_start|>system\nPlease reason step by step...
1       <|im_start|>system\nPlease reason step by step...
2       <|im_start|>system\nPlease reason step by step...
3       <|im_start|>system\nPlease reason step by step...
4       <|im_start|>system\nPlease reason step by step...
                              ...                        
9995    <|im_start|>system\nPlease reason step by step...
9996    <|im_start|>system\nPlease reason step by step...
9997    <|im_start|>system\nPlease reason step by step...
9998    <|im_start|>system\nPlease reason step by step...
9999    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 10000, dtype: object
self.prompt_key=prompt, self.response_key=response
0       <|im_start|>system\nPlease reason step by step...
1       <|im_start|>system\nPlease reason step by step...
2       <|im_start|>system\nPlease reason step by step...
3       <|im_start|>system\nPlease reason step by step...
4       <|im_start|>system\nPlease reason step by step...
                              ...                        
9995    <|im_start|>system\nPlease reason step by step...
9996    <|im_start|>system\nPlease reason step by step...
9997    <|im_start|>system\nPlease reason step by step...
9998    <|im_start|>system\nPlease reason step by step...
9999    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 10000, dtype: object
self.prompt_key=prompt, self.response_key=response
0      <|im_start|>system\nPlease reason step by step...
1      <|im_start|>system\nPlease reason step by step...
2      <|im_start|>system\nPlease reason step by step...
3      <|im_start|>system\nPlease reason step by step...
4      <|im_start|>system\nPlease reason step by step...
                             ...                        
611    <|im_start|>system\nPlease reason step by step...
612    <|im_start|>system\nPlease reason step by step...
613    <|im_start|>system\nPlease reason step by step...
614    <|im_start|>system\nPlease reason step by step...
615    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 616, dtype: object
self.prompt_key=prompt, self.response_key=response
0      <|im_start|>system\nPlease reason step by step...
1      <|im_start|>system\nPlease reason step by step...
2      <|im_start|>system\nPlease reason step by step...
3      <|im_start|>system\nPlease reason step by step...
4      <|im_start|>system\nPlease reason step by step...
                             ...                        
611    <|im_start|>system\nPlease reason step by step...
612    <|im_start|>system\nPlease reason step by step...
613    <|im_start|>system\nPlease reason step by step...
614    <|im_start|>system\nPlease reason step by step...
615    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 616, dtype: object
self.prompt_key=prompt, self.response_key=response
0       <|im_start|>system\nPlease reason step by step...
1       <|im_start|>system\nPlease reason step by step...
2       <|im_start|>system\nPlease reason step by step...
3       <|im_start|>system\nPlease reason step by step...
4       <|im_start|>system\nPlease reason step by step...
                              ...                        
9995    <|im_start|>system\nPlease reason step by step...
9996    <|im_start|>system\nPlease reason step by step...
9997    <|im_start|>system\nPlease reason step by step...
9998    <|im_start|>system\nPlease reason step by step...
9999    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 10000, dtype: object
self.prompt_key=prompt, self.response_key=response
0       <|im_start|>system\nPlease reason step by step...
1       <|im_start|>system\nPlease reason step by step...
2       <|im_start|>system\nPlease reason step by step...
3       <|im_start|>system\nPlease reason step by step...
4       <|im_start|>system\nPlease reason step by step...
                              ...                        
9995    <|im_start|>system\nPlease reason step by step...
9996    <|im_start|>system\nPlease reason step by step...
9997    <|im_start|>system\nPlease reason step by step...
9998    <|im_start|>system\nPlease reason step by step...
9999    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 10000, dtype: object
self.prompt_key=prompt, self.response_key=response
0       <|im_start|>system\nPlease reason step by step...
1       <|im_start|>system\nPlease reason step by step...
2       <|im_start|>system\nPlease reason step by step...
3       <|im_start|>system\nPlease reason step by step...
4       <|im_start|>system\nPlease reason step by step...
                              ...                        
9995    <|im_start|>system\nPlease reason step by step...
9996    <|im_start|>system\nPlease reason step by step...
9997    <|im_start|>system\nPlease reason step by step...
9998    <|im_start|>system\nPlease reason step by step...
9999    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 10000, dtype: object
self.prompt_key=prompt, self.response_key=response
0      <|im_start|>system\nPlease reason step by step...
1      <|im_start|>system\nPlease reason step by step...
2      <|im_start|>system\nPlease reason step by step...
3      <|im_start|>system\nPlease reason step by step...
4      <|im_start|>system\nPlease reason step by step...
                             ...                        
611    <|im_start|>system\nPlease reason step by step...
612    <|im_start|>system\nPlease reason step by step...
613    <|im_start|>system\nPlease reason step by step...
614    <|im_start|>system\nPlease reason step by step...
615    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 616, dtype: object
self.prompt_key=prompt, self.response_key=response
0      <|im_start|>system\nPlease reason step by step...
1      <|im_start|>system\nPlease reason step by step...
2      <|im_start|>system\nPlease reason step by step...
3      <|im_start|>system\nPlease reason step by step...
4      <|im_start|>system\nPlease reason step by step...
                             ...                        
611    <|im_start|>system\nPlease reason step by step...
612    <|im_start|>system\nPlease reason step by step...
613    <|im_start|>system\nPlease reason step by step...
614    <|im_start|>system\nPlease reason step by step...
615    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 616, dtype: object
self.prompt_key=prompt, self.response_key=response
0      <|im_start|>system\nPlease reason step by step...
1      <|im_start|>system\nPlease reason step by step...
2      <|im_start|>system\nPlease reason step by step...
3      <|im_start|>system\nPlease reason step by step...
4      <|im_start|>system\nPlease reason step by step...
                             ...                        
611    <|im_start|>system\nPlease reason step by step...
612    <|im_start|>system\nPlease reason step by step...
613    <|im_start|>system\nPlease reason step by step...
614    <|im_start|>system\nPlease reason step by step...
615    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 616, dtype: object
self.prompt_key=prompt, self.response_key=response
0       <|im_start|>system\nPlease reason step by step...
1       <|im_start|>system\nPlease reason step by step...
2       <|im_start|>system\nPlease reason step by step...
3       <|im_start|>system\nPlease reason step by step...
4       <|im_start|>system\nPlease reason step by step...
                              ...                        
9995    <|im_start|>system\nPlease reason step by step...
9996    <|im_start|>system\nPlease reason step by step...
9997    <|im_start|>system\nPlease reason step by step...
9998    <|im_start|>system\nPlease reason step by step...
9999    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 10000, dtype: object
self.prompt_key=prompt, self.response_key=response
0      <|im_start|>system\nPlease reason step by step...
1      <|im_start|>system\nPlease reason step by step...
2      <|im_start|>system\nPlease reason step by step...
3      <|im_start|>system\nPlease reason step by step...
4      <|im_start|>system\nPlease reason step by step...
                             ...                        
611    <|im_start|>system\nPlease reason step by step...
612    <|im_start|>system\nPlease reason step by step...
613    <|im_start|>system\nPlease reason step by step...
614    <|im_start|>system\nPlease reason step by step...
615    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 616, dtype: object
self.prompt_key=prompt, self.response_key=response
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
Normalize batch size by dp 8
Using sequence parallel size: 1
Using remove padding: False
0       <|im_start|>system\nPlease reason step by step...
1       <|im_start|>system\nPlease reason step by step...
2       <|im_start|>system\nPlease reason step by step...
3       <|im_start|>system\nPlease reason step by step...
4       <|im_start|>system\nPlease reason step by step...
                              ...                        
9995    <|im_start|>system\nPlease reason step by step...
9996    <|im_start|>system\nPlease reason step by step...
9997    <|im_start|>system\nPlease reason step by step...
9998    <|im_start|>system\nPlease reason step by step...
9999    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 10000, dtype: object
self.prompt_key=prompt, self.response_key=response
0      <|im_start|>system\nPlease reason step by step...
1      <|im_start|>system\nPlease reason step by step...
2      <|im_start|>system\nPlease reason step by step...
3      <|im_start|>system\nPlease reason step by step...
4      <|im_start|>system\nPlease reason step by step...
                             ...                        
611    <|im_start|>system\nPlease reason step by step...
612    <|im_start|>system\nPlease reason step by step...
613    <|im_start|>system\nPlease reason step by step...
614    <|im_start|>system\nPlease reason step by step...
615    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 616, dtype: object
self.prompt_key=prompt, self.response_key=response
Using FSDP rank 0 and size 8 for data distribution
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
0       <|im_start|>system\nPlease reason step by step...
1       <|im_start|>system\nPlease reason step by step...
2       <|im_start|>system\nPlease reason step by step...
3       <|im_start|>system\nPlease reason step by step...
4       <|im_start|>system\nPlease reason step by step...
                              ...                        
9995    <|im_start|>system\nPlease reason step by step...
9996    <|im_start|>system\nPlease reason step by step...
9997    <|im_start|>system\nPlease reason step by step...
9998    <|im_start|>system\nPlease reason step by step...
9999    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 10000, dtype: object
self.prompt_key=prompt, self.response_key=response
0      <|im_start|>system\nPlease reason step by step...
1      <|im_start|>system\nPlease reason step by step...
2      <|im_start|>system\nPlease reason step by step...
3      <|im_start|>system\nPlease reason step by step...
4      <|im_start|>system\nPlease reason step by step...
                             ...                        
611    <|im_start|>system\nPlease reason step by step...
612    <|im_start|>system\nPlease reason step by step...
613    <|im_start|>system\nPlease reason step by step...
614    <|im_start|>system\nPlease reason step by step...
615    <|im_start|>system\nPlease reason step by step...
Name: prompt, Length: 616, dtype: object
self.prompt_key=prompt, self.response_key=response
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
functools.partial(<function _or_policy at 0x7faf2b1b1240>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7faf2b1b1120>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
NCCL version 2.20.5+cuda12.4
Total training steps: 78
Total training steps: 78
Total training steps: 78
Epoch 1/2:   0%|          | 0/39 [00:00<?, ?it/s]Total training steps: 78
Epoch 1/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 1/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 1/2:   0%|          | 0/39 [00:00<?, ?it/s]Total training steps: 78
Number of steps/epoch 39, number of epochs 2, total number of steps 78
{'data': {'train_batch_size': 32, 'micro_batch_size': None, 'micro_batch_size_per_gpu': 2, 'train_files': 'data/train/pi1_r128_responses_16000.parquet', 'val_files': 'data/train/pi1_r128_responses_16000_valid.parquet', 'prompt_key': 'prompt', 'response_key': 'response', 'max_length': 1920, 'truncation': 'error', 'balance_dp_token': False, 'chat_template': None}, 'model': {'partial_pretrain': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B', 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'cpu_offload': False, 'offload_params': False}, 'external_lib': None, 'enable_gradient_checkpointing': False, 'trust_remote_code': False, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear', 'use_liger': False}, 'optim': {'lr': 1e-05, 'betas': [0.9, 0.95], 'weight_decay': 0.01, 'warmup_steps_ratio': 0.1, 'clip_grad': 1.0}, 'ulysses_sequence_parallel_size': 1, 'use_remove_padding': False, 'trainer': {'default_local_dir': '/local1/lxh22/save/offline_grpo/1.5b_pi1_sft', 'default_hdfs_dir': None, 'resume_path': None, 'project_name': 'sft', 'experiment_name': '1.5b_pi1_sft_0814-0057', 'total_epochs': 2, 'total_training_steps': None, 'logger': ['console', 'wandb'], 'seed': 1}}
Total training steps: 78
Epoch 1/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 1/2:   0%|          | 0/39 [00:00<?, ?it/s]Total training steps: 78
Epoch 1/2:   0%|          | 0/39 [00:00<?, ?it/s]wandb: Currently logged in as: lixinghan2013 (coder66-RL-lab) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.20.1
wandb: Run data is saved locally in /homes/gws/lxh22/rl-sft/rl-tests/wandb/run-20250814_005806-wbfgzpqf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 1.5b_pi1_sft_0814-0057
wandb: â­ï¸ View project at https://wandb.ai/coder66-RL-lab/sft
wandb: ðŸš€ View run at https://wandb.ai/coder66-RL-lab/sft/runs/wbfgzpqf
Using LocalLogger is deprecated. The constructor API will change 
Total training steps: 78
Epoch 1/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 1/2:   3%|â–Ž         | 1/39 [00:25<15:53, 25.08s/it]Epoch 1/2:   3%|â–Ž         | 1/39 [00:25<15:53, 25.10s/it]Epoch 1/2:   3%|â–Ž         | 1/39 [00:25<15:52, 25.07s/it]Epoch 1/2:   3%|â–Ž         | 1/39 [00:25<15:53, 25.10s/it]step:0 - train/loss:0.126 - train/lr(1e-3):0.001
Epoch 1/2:   3%|â–Ž         | 1/39 [00:22<14:02, 22.18s/it]Epoch 1/2:   3%|â–Ž         | 1/39 [00:25<15:53, 25.09s/it]Epoch 1/2:   3%|â–Ž         | 1/39 [00:25<15:53, 25.10s/it]Epoch 1/2:   3%|â–Ž         | 1/39 [00:25<15:53, 25.10s/it]Epoch 1/2:   5%|â–Œ         | 2/39 [00:46<14:02, 22.78s/it]step:1 - train/loss:0.131 - train/lr(1e-3):0.003
Epoch 1/2:   5%|â–Œ         | 2/39 [00:46<14:02, 22.78s/it]Epoch 1/2:   5%|â–Œ         | 2/39 [00:43<13:18, 21.58s/it]Epoch 1/2:   5%|â–Œ         | 2/39 [00:46<14:02, 22.77s/it]Epoch 1/2:   5%|â–Œ         | 2/39 [00:46<14:02, 22.78s/it]Epoch 1/2:   5%|â–Œ         | 2/39 [00:46<14:02, 22.77s/it]Epoch 1/2:   5%|â–Œ         | 2/39 [00:46<14:02, 22.77s/it]Epoch 1/2:   5%|â–Œ         | 2/39 [00:46<14:02, 22.78s/it]Epoch 1/2:   8%|â–Š         | 3/39 [01:07<13:12, 22.00s/it]step:2 - train/loss:0.125 - train/lr(1e-3):0.004
Epoch 1/2:   8%|â–Š         | 3/39 [01:04<12:48, 21.35s/it]Epoch 1/2:   8%|â–Š         | 3/39 [01:07<13:11, 22.00s/it]Epoch 1/2:   8%|â–Š         | 3/39 [01:07<13:12, 22.01s/it]Epoch 1/2:   8%|â–Š         | 3/39 [01:07<13:12, 22.00s/it]Epoch 1/2:   8%|â–Š         | 3/39 [01:07<13:12, 22.00s/it]Epoch 1/2:   8%|â–Š         | 3/39 [01:07<13:12, 22.00s/it]Epoch 1/2:   8%|â–Š         | 3/39 [01:07<13:12, 22.00s/it]Epoch 1/2:  10%|â–ˆ         | 4/39 [01:28<12:38, 21.66s/it]step:3 - train/loss:0.129 - train/lr(1e-3):0.006
Epoch 1/2:  10%|â–ˆ         | 4/39 [01:28<12:38, 21.66s/it]Epoch 1/2:  10%|â–ˆ         | 4/39 [01:25<12:24, 21.27s/it]Epoch 1/2:  10%|â–ˆ         | 4/39 [01:28<12:38, 21.66s/it]Epoch 1/2:  10%|â–ˆ         | 4/39 [01:28<12:38, 21.66s/it]Epoch 1/2:  10%|â–ˆ         | 4/39 [01:28<12:38, 21.66s/it]Epoch 1/2:  10%|â–ˆ         | 4/39 [01:28<12:38, 21.66s/it]Epoch 1/2:  10%|â–ˆ         | 4/39 [01:28<12:38, 21.66s/it]Epoch 1/2:  13%|â–ˆâ–Ž        | 5/39 [01:49<12:09, 21.46s/it]step:4 - train/loss:0.121 - train/lr(1e-3):0.007Epoch 1/2:  13%|â–ˆâ–Ž        | 5/39 [01:49<12:09, 21.46s/it]
Epoch 1/2:  13%|â–ˆâ–Ž        | 5/39 [01:46<12:01, 21.21s/it]Epoch 1/2:  13%|â–ˆâ–Ž        | 5/39 [01:49<12:09, 21.46s/it]Epoch 1/2:  13%|â–ˆâ–Ž        | 5/39 [01:49<12:09, 21.46s/it]Epoch 1/2:  13%|â–ˆâ–Ž        | 5/39 [01:49<12:09, 21.46s/it]Epoch 1/2:  13%|â–ˆâ–Ž        | 5/39 [01:49<12:09, 21.46s/it]Epoch 1/2:  13%|â–ˆâ–Ž        | 5/39 [01:49<12:09, 21.46s/it]Epoch 1/2:  15%|â–ˆâ–Œ        | 6/39 [02:10<11:44, 21.35s/it]Epoch 1/2:  15%|â–ˆâ–Œ        | 6/39 [02:10<11:44, 21.35s/it]step:5 - train/loss:0.126 - train/lr(1e-3):0.009
Epoch 1/2:  15%|â–ˆâ–Œ        | 6/39 [02:07<11:39, 21.18s/it]Epoch 1/2:  15%|â–ˆâ–Œ        | 6/39 [02:10<11:44, 21.35s/it]Epoch 1/2:  15%|â–ˆâ–Œ        | 6/39 [02:10<11:44, 21.35s/it]Epoch 1/2:  15%|â–ˆâ–Œ        | 6/39 [02:10<11:44, 21.35s/it]Epoch 1/2:  15%|â–ˆâ–Œ        | 6/39 [02:10<11:44, 21.35s/it]Epoch 1/2:  15%|â–ˆâ–Œ        | 6/39 [02:10<11:44, 21.35s/it]Epoch 1/2:  18%|â–ˆâ–Š        | 7/39 [02:32<11:25, 21.41s/it]Epoch 1/2:  18%|â–ˆâ–Š        | 7/39 [02:32<11:25, 21.41s/it]step:6 - train/loss:0.116 - train/lr(1e-3):0.010
Epoch 1/2:  18%|â–ˆâ–Š        | 7/39 [02:29<11:21, 21.30s/it]Epoch 1/2:  18%|â–ˆâ–Š        | 7/39 [02:32<11:25, 21.41s/it]Epoch 1/2:  18%|â–ˆâ–Š        | 7/39 [02:32<11:25, 21.41s/it]Epoch 1/2:  18%|â–ˆâ–Š        | 7/39 [02:32<11:25, 21.41s/it]Epoch 1/2:  18%|â–ˆâ–Š        | 7/39 [02:32<11:25, 21.41s/it]Epoch 1/2:  18%|â–ˆâ–Š        | 7/39 [02:32<11:25, 21.41s/it]step:7 - train/loss:0.120 - train/lr(1e-3):0.010Epoch 1/2:  21%|â–ˆâ–ˆ        | 8/39 [02:53<11:01, 21.33s/it]
Epoch 1/2:  21%|â–ˆâ–ˆ        | 8/39 [02:53<11:01, 21.33s/it]Epoch 1/2:  21%|â–ˆâ–ˆ        | 8/39 [02:50<10:58, 21.25s/it]Epoch 1/2:  21%|â–ˆâ–ˆ        | 8/39 [02:53<11:01, 21.33s/it]Epoch 1/2:  21%|â–ˆâ–ˆ        | 8/39 [02:53<11:01, 21.33s/it]Epoch 1/2:  21%|â–ˆâ–ˆ        | 8/39 [02:53<11:01, 21.33s/it]Epoch 1/2:  21%|â–ˆâ–ˆ        | 8/39 [02:53<11:01, 21.33s/it]Epoch 1/2:  21%|â–ˆâ–ˆ        | 8/39 [02:53<11:01, 21.33s/it]Epoch 1/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:14<10:38, 21.27s/it]Epoch 1/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:14<10:38, 21.27s/it]step:8 - train/loss:0.127 - train/lr(1e-3):0.010
Epoch 1/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:14<10:38, 21.27s/it]Epoch 1/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:14<10:38, 21.27s/it]Epoch 1/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:14<10:38, 21.27s/it]Epoch 1/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:14<10:38, 21.27s/it]Epoch 1/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:11<10:36, 21.21s/it]Epoch 1/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:14<10:38, 21.27s/it]Epoch 1/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:35<10:15, 21.23s/it]step:9 - train/loss:0.126 - train/lr(1e-3):0.010
Epoch 1/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:35<10:15, 21.23s/it]Epoch 1/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:32<10:14, 21.19s/it]Epoch 1/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:35<10:15, 21.23s/it]Epoch 1/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:35<10:15, 21.23s/it]Epoch 1/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:35<10:15, 21.23s/it]Epoch 1/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:35<10:15, 21.23s/it]Epoch 1/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:35<10:15, 21.23s/it]Epoch 1/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:56<09:53, 21.20s/it]step:10 - train/loss:0.121 - train/lr(1e-3):0.010Epoch 1/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:56<09:53, 21.20s/it]
Epoch 1/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:53<09:52, 21.17s/it]Epoch 1/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:56<09:53, 21.20s/it]Epoch 1/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:56<09:53, 21.20s/it]Epoch 1/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:56<09:53, 21.20s/it]Epoch 1/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:56<09:53, 21.20s/it]Epoch 1/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:56<09:53, 21.20s/it]Epoch 1/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:17<09:31, 21.18s/it]Epoch 1/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:17<09:31, 21.18s/it]step:11 - train/loss:0.113 - train/lr(1e-3):0.010
Epoch 1/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:15<09:31, 21.16s/it]Epoch 1/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:17<09:31, 21.18s/it]Epoch 1/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:17<09:31, 21.18s/it]Epoch 1/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:17<09:31, 21.18s/it]Epoch 1/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:17<09:31, 21.18s/it]Epoch 1/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:17<09:31, 21.18s/it]Epoch 1/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:39<09:10, 21.16s/it]step:12 - train/loss:0.117 - train/lr(1e-3):0.010Epoch 1/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:39<09:10, 21.16s/it]
Epoch 1/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:36<09:09, 21.14s/it]Epoch 1/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:39<09:10, 21.16s/it]Epoch 1/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:39<09:10, 21.16s/it]Epoch 1/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:39<09:10, 21.16s/it]Epoch 1/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:39<09:10, 21.16s/it]Epoch 1/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:39<09:10, 21.16s/it]step:13 - train/loss:0.116 - train/lr(1e-3):0.010Epoch 1/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [05:00<08:48, 21.15s/it]
Epoch 1/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [05:00<08:48, 21.15s/it]Epoch 1/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [04:57<08:48, 21.14s/it]Epoch 1/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [05:00<08:48, 21.15s/it]Epoch 1/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [05:00<08:48, 21.15s/it]Epoch 1/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [05:00<08:48, 21.15s/it]Epoch 1/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [05:00<08:48, 21.15s/it]Epoch 1/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [05:00<08:48, 21.15s/it]Epoch 1/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:21<08:27, 21.15s/it]step:14 - train/loss:0.118 - train/lr(1e-3):0.010Epoch 1/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:21<08:27, 21.15s/it]
Epoch 1/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:18<08:27, 21.14s/it]Epoch 1/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:21<08:27, 21.15s/it]Epoch 1/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:21<08:27, 21.15s/it]Epoch 1/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:21<08:27, 21.15s/it]Epoch 1/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:21<08:27, 21.15s/it]Epoch 1/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:21<08:27, 21.15s/it]Epoch 1/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:42<08:06, 21.14s/it]step:15 - train/loss:0.112 - train/lr(1e-3):0.010Epoch 1/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:42<08:06, 21.14s/it]
Epoch 1/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:39<08:06, 21.14s/it]Epoch 1/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:42<08:06, 21.14s/it]Epoch 1/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:42<08:06, 21.14s/it]Epoch 1/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:42<08:06, 21.14s/it]Epoch 1/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:42<08:06, 21.14s/it]Epoch 1/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:42<08:06, 21.14s/it]step:16 - train/loss:0.120 - train/lr(1e-3):0.010Epoch 1/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [06:03<07:45, 21.14s/it]Epoch 1/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [06:03<07:45, 21.14s/it]
Epoch 1/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [06:00<07:44, 21.13s/it]Epoch 1/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [06:03<07:45, 21.14s/it]Epoch 1/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [06:03<07:45, 21.14s/it]Epoch 1/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [06:03<07:45, 21.14s/it]Epoch 1/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [06:03<07:45, 21.14s/it]Epoch 1/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [06:03<07:45, 21.14s/it]step:17 - train/loss:0.111 - train/lr(1e-3):0.009Epoch 1/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:24<07:23, 21.13s/it]Epoch 1/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:24<07:23, 21.13s/it]
Epoch 1/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:21<07:23, 21.13s/it]Epoch 1/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:24<07:23, 21.13s/it]Epoch 1/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:24<07:23, 21.13s/it]Epoch 1/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:24<07:23, 21.13s/it]Epoch 1/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:24<07:23, 21.13s/it]Epoch 1/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:24<07:23, 21.13s/it]step:18 - train/loss:0.110 - train/lr(1e-3):0.009Epoch 1/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:45<07:02, 21.13s/it]Epoch 1/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:45<07:02, 21.13s/it]
Epoch 1/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:42<07:02, 21.13s/it]Epoch 1/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:45<07:02, 21.13s/it]Epoch 1/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:45<07:02, 21.13s/it]Epoch 1/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:45<07:02, 21.13s/it]Epoch 1/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:45<07:02, 21.13s/it]Epoch 1/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:45<07:02, 21.13s/it]Epoch 1/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:06<06:41, 21.14s/it]Epoch 1/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:06<06:41, 21.14s/it]step:19 - train/loss:0.114 - train/lr(1e-3):0.009
Epoch 1/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:04<06:41, 21.14s/it]Epoch 1/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:06<06:41, 21.14s/it]Epoch 1/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:06<06:41, 21.14s/it]Epoch 1/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:07<06:41, 21.14s/it]Epoch 1/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:06<06:41, 21.14s/it]Epoch 1/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:06<06:41, 21.14s/it]step:20 - train/loss:0.113 - train/lr(1e-3):0.009Epoch 1/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:28<06:20, 21.13s/it]
Epoch 1/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:28<06:20, 21.13s/it]Epoch 1/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:25<06:20, 21.13s/it]Epoch 1/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:28<06:20, 21.13s/it]Epoch 1/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:28<06:20, 21.13s/it]Epoch 1/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:28<06:20, 21.13s/it]Epoch 1/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:28<06:20, 21.13s/it]Epoch 1/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:28<06:20, 21.13s/it]Epoch 1/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:49<05:59, 21.14s/it]step:21 - train/loss:0.110 - train/lr(1e-3):0.009Epoch 1/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:49<05:59, 21.14s/it]
Epoch 1/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:46<05:59, 21.14s/it]Epoch 1/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:49<05:59, 21.14s/it]Epoch 1/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:49<05:59, 21.14s/it]Epoch 1/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:49<05:59, 21.14s/it]Epoch 1/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:49<05:59, 21.14s/it]Epoch 1/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:49<05:59, 21.14s/it]Epoch 1/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:10<05:39, 21.20s/it]step:22 - train/loss:0.114 - train/lr(1e-3):0.009Epoch 1/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:10<05:39, 21.20s/it]
Epoch 1/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:07<05:39, 21.20s/it]Epoch 1/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:10<05:39, 21.20s/it]Epoch 1/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:10<05:39, 21.20s/it]Epoch 1/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:10<05:39, 21.20s/it]Epoch 1/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:10<05:39, 21.20s/it]Epoch 1/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:10<05:39, 21.20s/it]step:23 - train/loss:0.114 - train/lr(1e-3):0.009Epoch 1/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:31<05:17, 21.18s/it]Epoch 1/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:31<05:17, 21.18s/it]
Epoch 1/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:28<05:17, 21.18s/it]Epoch 1/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:31<05:17, 21.18s/it]Epoch 1/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:31<05:17, 21.18s/it]Epoch 1/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:31<05:17, 21.18s/it]Epoch 1/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:31<05:17, 21.18s/it]Epoch 1/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:31<05:17, 21.18s/it]step:24 - train/loss:0.114 - train/lr(1e-3):0.008Epoch 1/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:52<04:56, 21.16s/it]Epoch 1/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:52<04:56, 21.16s/it]
Epoch 1/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:49<04:56, 21.16s/it]Epoch 1/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:52<04:56, 21.16s/it]Epoch 1/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:52<04:56, 21.16s/it]Epoch 1/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:52<04:56, 21.16s/it]Epoch 1/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:52<04:56, 21.16s/it]Epoch 1/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:52<04:56, 21.16s/it]step:25 - train/loss:0.118 - train/lr(1e-3):0.008
Epoch 1/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:13<04:35, 21.15s/it]Epoch 1/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:13<04:35, 21.15s/it]Epoch 1/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:11<04:35, 21.15s/it]Epoch 1/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:13<04:35, 21.15s/it]Epoch 1/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:13<04:35, 21.15s/it]Epoch 1/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:13<04:35, 21.15s/it]Epoch 1/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:13<04:35, 21.15s/it]Epoch 1/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:13<04:35, 21.15s/it]Epoch 1/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:35<04:13, 21.15s/it]Epoch 1/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:35<04:13, 21.15s/it]step:26 - train/loss:0.111 - train/lr(1e-3):0.008
Epoch 1/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:32<04:13, 21.15s/it]Epoch 1/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:35<04:13, 21.15s/it]Epoch 1/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:35<04:13, 21.15s/it]Epoch 1/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:35<04:13, 21.15s/it]Epoch 1/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:35<04:13, 21.15s/it]Epoch 1/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:35<04:13, 21.15s/it]step:27 - train/loss:0.110 - train/lr(1e-3):0.008Epoch 1/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:56<03:52, 21.15s/it]Epoch 1/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:56<03:52, 21.15s/it]
Epoch 1/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:53<03:52, 21.15s/it]Epoch 1/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:56<03:52, 21.15s/it]Epoch 1/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:56<03:52, 21.15s/it]Epoch 1/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:56<03:52, 21.15s/it]Epoch 1/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:56<03:52, 21.15s/it]Epoch 1/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:56<03:52, 21.15s/it]Epoch 1/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:17<03:31, 21.14s/it]Epoch 1/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:17<03:31, 21.14s/it]step:28 - train/loss:0.114 - train/lr(1e-3):0.008
Epoch 1/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:14<03:31, 21.14s/it]Epoch 1/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:17<03:31, 21.14s/it]Epoch 1/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:17<03:31, 21.14s/it]Epoch 1/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:17<03:31, 21.14s/it]Epoch 1/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:17<03:31, 21.14s/it]Epoch 1/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:17<03:31, 21.14s/it]step:29 - train/loss:0.116 - train/lr(1e-3):0.008Epoch 1/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:38<03:10, 21.14s/it]
Epoch 1/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:38<03:10, 21.14s/it]Epoch 1/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:35<03:10, 21.14s/it]Epoch 1/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:38<03:10, 21.14s/it]Epoch 1/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:38<03:10, 21.14s/it]Epoch 1/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:38<03:10, 21.14s/it]Epoch 1/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:38<03:10, 21.14s/it]Epoch 1/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:38<03:10, 21.14s/it]Epoch 1/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:59<02:49, 21.13s/it]step:30 - train/loss:0.114 - train/lr(1e-3):0.007Epoch 1/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:59<02:49, 21.13s/it]
Epoch 1/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:56<02:49, 21.13s/it]Epoch 1/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:59<02:49, 21.13s/it]Epoch 1/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:59<02:49, 21.13s/it]Epoch 1/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:59<02:49, 21.13s/it]Epoch 1/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:59<02:49, 21.13s/it]Epoch 1/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:59<02:49, 21.13s/it]Epoch 1/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:20<02:28, 21.14s/it]Epoch 1/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:20<02:28, 21.14s/it]Epoch 1/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:20<02:28, 21.14s/it]Epoch 1/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:20<02:28, 21.14s/it]Epoch 1/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:20<02:28, 21.14s/it]Epoch 1/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:20<02:28, 21.14s/it]Epoch 1/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:20<02:28, 21.14s/it]step:31 - train/loss:0.111 - train/lr(1e-3):0.007
Epoch 1/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:17<02:28, 21.14s/it]Epoch 1/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:41<02:06, 21.13s/it]Epoch 1/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:41<02:06, 21.13s/it]step:32 - train/loss:0.115 - train/lr(1e-3):0.007
Epoch 1/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:39<02:06, 21.13s/it]Epoch 1/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:41<02:06, 21.13s/it]Epoch 1/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:41<02:06, 21.13s/it]Epoch 1/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:41<02:06, 21.13s/it]Epoch 1/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:41<02:06, 21.13s/it]Epoch 1/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:41<02:06, 21.13s/it]Epoch 1/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [12:03<01:45, 21.20s/it]Epoch 1/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [12:03<01:45, 21.20s/it]step:33 - train/loss:0.113 - train/lr(1e-3):0.007
Epoch 1/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [12:00<01:45, 21.20s/it]Epoch 1/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [12:03<01:45, 21.20s/it]Epoch 1/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [12:03<01:45, 21.20s/it]Epoch 1/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [12:03<01:45, 21.20s/it]Epoch 1/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [12:03<01:45, 21.20s/it]Epoch 1/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [12:03<01:45, 21.20s/it]Epoch 1/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:24<01:24, 21.17s/it]step:34 - train/loss:0.108 - train/lr(1e-3):0.007Epoch 1/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:24<01:24, 21.17s/it]
Epoch 1/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:21<01:24, 21.17s/it]Epoch 1/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:24<01:24, 21.17s/it]Epoch 1/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:24<01:24, 21.17s/it]Epoch 1/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:24<01:24, 21.17s/it]Epoch 1/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:24<01:24, 21.17s/it]Epoch 1/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:24<01:24, 21.17s/it]Epoch 1/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:45<01:03, 21.16s/it]step:35 - train/loss:0.115 - train/lr(1e-3):0.006Epoch 1/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:45<01:03, 21.16s/it]
Epoch 1/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:42<01:03, 21.16s/it]Epoch 1/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:45<01:03, 21.16s/it]Epoch 1/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:45<01:03, 21.16s/it]Epoch 1/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:45<01:03, 21.16s/it]Epoch 1/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:45<01:03, 21.16s/it]Epoch 1/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:45<01:03, 21.16s/it]Epoch 1/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:06<00:42, 21.15s/it]Epoch 1/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:06<00:42, 21.15s/it]step:36 - train/loss:0.114 - train/lr(1e-3):0.006
Epoch 1/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:03<00:42, 21.15s/it]Epoch 1/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:06<00:42, 21.15s/it]Epoch 1/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:06<00:42, 21.15s/it]Epoch 1/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:06<00:42, 21.15s/it]Epoch 1/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:06<00:42, 21.15s/it]Epoch 1/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:06<00:42, 21.15s/it]step:37 - train/loss:0.112 - train/lr(1e-3):0.006Epoch 1/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:27<00:21, 21.15s/it]Epoch 1/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:27<00:21, 21.15s/it]
Epoch 1/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:24<00:21, 21.15s/it]Epoch 1/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:27<00:21, 21.15s/it]Epoch 1/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:27<00:21, 21.15s/it]Epoch 1/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:27<00:21, 21.15s/it]Epoch 1/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:27<00:21, 21.15s/it]Epoch 1/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:27<00:21, 21.15s/it]Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:48<00:00, 21.15s/it]step:38 - train/loss:0.113 - train/lr(1e-3):0.006Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:48<00:00, 21.15s/it]
Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:48<00:00, 21.15s/it]Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:46<00:00, 21.15s/it]Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:48<00:00, 21.15s/it]Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:48<00:00, 21.15s/it]Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:48<00:00, 21.15s/it]Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:48<00:00, 21.15s/it]Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:46<00:00, 21.19s/it]
Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:49<00:00, 21.26s/it]
Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:49<00:00, 21.26s/it]
Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:49<00:00, 21.26s/it]
Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:49<00:00, 21.26s/it]
Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:49<00:00, 21.26s/it]
Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:49<00:00, 21.26s/it]
Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [13:49<00:00, 21.26s/it]
step:39 - val/loss:0.111
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
Epoch 2/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/39 [00:00<?, ?it/s]Epoch 2/2:   3%|â–Ž         | 1/39 [00:21<13:44, 21.70s/it]Epoch 2/2:   3%|â–Ž         | 1/39 [00:21<13:44, 21.70s/it]Epoch 2/2:   3%|â–Ž         | 1/39 [00:21<13:44, 21.70s/it]Epoch 2/2:   3%|â–Ž         | 1/39 [00:21<13:44, 21.70s/it]Epoch 2/2:   3%|â–Ž         | 1/39 [00:21<13:44, 21.70s/it]Epoch 2/2:   3%|â–Ž         | 1/39 [00:21<13:44, 21.70s/it]Epoch 2/2:   3%|â–Ž         | 1/39 [00:21<13:44, 21.70s/it]step:39 - train/loss:0.107 - train/lr(1e-3):0.006
Epoch 2/2:   3%|â–Ž         | 1/39 [00:21<13:42, 21.65s/it]Epoch 2/2:   5%|â–Œ         | 2/39 [00:42<13:10, 21.36s/it]Epoch 2/2:   5%|â–Œ         | 2/39 [00:42<13:10, 21.37s/it]Epoch 2/2:   5%|â–Œ         | 2/39 [00:42<13:10, 21.37s/it]Epoch 2/2:   5%|â–Œ         | 2/39 [00:42<13:10, 21.37s/it]Epoch 2/2:   5%|â–Œ         | 2/39 [00:42<13:10, 21.37s/it]step:40 - train/loss:0.110 - train/lr(1e-3):0.005Epoch 2/2:   5%|â–Œ         | 2/39 [00:42<13:10, 21.37s/it]
Epoch 2/2:   5%|â–Œ         | 2/39 [00:42<13:10, 21.37s/it]Epoch 2/2:   5%|â–Œ         | 2/39 [00:42<13:09, 21.34s/it]Epoch 2/2:   8%|â–Š         | 3/39 [01:03<12:45, 21.25s/it]Epoch 2/2:   8%|â–Š         | 3/39 [01:03<12:45, 21.25s/it]Epoch 2/2:   8%|â–Š         | 3/39 [01:03<12:45, 21.25s/it]Epoch 2/2:   8%|â–Š         | 3/39 [01:03<12:45, 21.25s/it]Epoch 2/2:   8%|â–Š         | 3/39 [01:03<12:45, 21.25s/it]Epoch 2/2:   8%|â–Š         | 3/39 [01:03<12:45, 21.25s/it]Epoch 2/2:   8%|â–Š         | 3/39 [01:03<12:45, 21.25s/it]step:41 - train/loss:0.110 - train/lr(1e-3):0.005
Epoch 2/2:   8%|â–Š         | 3/39 [01:03<12:44, 21.24s/it]Epoch 2/2:  10%|â–ˆ         | 4/39 [01:25<12:21, 21.19s/it]Epoch 2/2:  10%|â–ˆ         | 4/39 [01:25<12:21, 21.19s/it]Epoch 2/2:  10%|â–ˆ         | 4/39 [01:25<12:21, 21.19s/it]Epoch 2/2:  10%|â–ˆ         | 4/39 [01:25<12:21, 21.19s/it]Epoch 2/2:  10%|â–ˆ         | 4/39 [01:25<12:21, 21.19s/it]Epoch 2/2:  10%|â–ˆ         | 4/39 [01:25<12:21, 21.19s/it]step:42 - train/loss:0.111 - train/lr(1e-3):0.005Epoch 2/2:  10%|â–ˆ         | 4/39 [01:25<12:21, 21.19s/it]
Epoch 2/2:  10%|â–ˆ         | 4/39 [01:24<12:21, 21.18s/it]Epoch 2/2:  13%|â–ˆâ–Ž        | 5/39 [01:46<11:59, 21.17s/it]Epoch 2/2:  13%|â–ˆâ–Ž        | 5/39 [01:46<11:59, 21.17s/it]Epoch 2/2:  13%|â–ˆâ–Ž        | 5/39 [01:46<11:59, 21.17s/it]Epoch 2/2:  13%|â–ˆâ–Ž        | 5/39 [01:46<11:59, 21.17s/it]Epoch 2/2:  13%|â–ˆâ–Ž        | 5/39 [01:46<11:59, 21.17s/it]Epoch 2/2:  13%|â–ˆâ–Ž        | 5/39 [01:46<11:59, 21.17s/it]step:43 - train/loss:0.108 - train/lr(1e-3):0.005
Epoch 2/2:  13%|â–ˆâ–Ž        | 5/39 [01:46<11:59, 21.17s/it]Epoch 2/2:  13%|â–ˆâ–Ž        | 5/39 [01:46<11:59, 21.17s/it]Epoch 2/2:  15%|â–ˆâ–Œ        | 6/39 [02:07<11:37, 21.15s/it]Epoch 2/2:  15%|â–ˆâ–Œ        | 6/39 [02:07<11:37, 21.15s/it]Epoch 2/2:  15%|â–ˆâ–Œ        | 6/39 [02:07<11:37, 21.15s/it]Epoch 2/2:  15%|â–ˆâ–Œ        | 6/39 [02:07<11:37, 21.15s/it]Epoch 2/2:  15%|â–ˆâ–Œ        | 6/39 [02:07<11:37, 21.15s/it]Epoch 2/2:  15%|â–ˆâ–Œ        | 6/39 [02:07<11:37, 21.15s/it]step:44 - train/loss:0.113 - train/lr(1e-3):0.004Epoch 2/2:  15%|â–ˆâ–Œ        | 6/39 [02:07<11:37, 21.15s/it]
Epoch 2/2:  15%|â–ˆâ–Œ        | 6/39 [02:07<11:37, 21.15s/it]Epoch 2/2:  18%|â–ˆâ–Š        | 7/39 [02:28<11:16, 21.14s/it]Epoch 2/2:  18%|â–ˆâ–Š        | 7/39 [02:28<11:16, 21.14s/it]Epoch 2/2:  18%|â–ˆâ–Š        | 7/39 [02:28<11:16, 21.14s/it]step:45 - train/loss:0.113 - train/lr(1e-3):0.004Epoch 2/2:  18%|â–ˆâ–Š        | 7/39 [02:28<11:16, 21.14s/it]Epoch 2/2:  18%|â–ˆâ–Š        | 7/39 [02:28<11:16, 21.14s/it]Epoch 2/2:  18%|â–ˆâ–Š        | 7/39 [02:28<11:16, 21.14s/it]
Epoch 2/2:  18%|â–ˆâ–Š        | 7/39 [02:28<11:16, 21.14s/it]Epoch 2/2:  18%|â–ˆâ–Š        | 7/39 [02:28<11:16, 21.14s/it]Epoch 2/2:  21%|â–ˆâ–ˆ        | 8/39 [02:49<10:55, 21.14s/it]Epoch 2/2:  21%|â–ˆâ–ˆ        | 8/39 [02:49<10:55, 21.14s/it]Epoch 2/2:  21%|â–ˆâ–ˆ        | 8/39 [02:49<10:55, 21.14s/it]Epoch 2/2:  21%|â–ˆâ–ˆ        | 8/39 [02:49<10:55, 21.14s/it]step:46 - train/loss:0.111 - train/lr(1e-3):0.004Epoch 2/2:  21%|â–ˆâ–ˆ        | 8/39 [02:49<10:55, 21.14s/it]Epoch 2/2:  21%|â–ˆâ–ˆ        | 8/39 [02:49<10:55, 21.14s/it]
Epoch 2/2:  21%|â–ˆâ–ˆ        | 8/39 [02:49<10:55, 21.14s/it]Epoch 2/2:  21%|â–ˆâ–ˆ        | 8/39 [02:49<10:55, 21.14s/it]Epoch 2/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:10<10:33, 21.13s/it]Epoch 2/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:10<10:33, 21.13s/it]Epoch 2/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:10<10:33, 21.13s/it]Epoch 2/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:10<10:33, 21.13s/it]Epoch 2/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:10<10:33, 21.13s/it]Epoch 2/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:10<10:33, 21.13s/it]Epoch 2/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:10<10:33, 21.13s/it]step:47 - train/loss:0.107 - train/lr(1e-3):0.004
Epoch 2/2:  23%|â–ˆâ–ˆâ–Ž       | 9/39 [03:10<10:33, 21.13s/it]Epoch 2/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:31<10:12, 21.13s/it]Epoch 2/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:31<10:12, 21.13s/it]step:48 - train/loss:0.110 - train/lr(1e-3):0.004Epoch 2/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:31<10:12, 21.13s/it]Epoch 2/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:31<10:12, 21.13s/it]Epoch 2/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:31<10:12, 21.13s/it]
Epoch 2/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:31<10:12, 21.13s/it]Epoch 2/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:31<10:12, 21.13s/it]Epoch 2/2:  26%|â–ˆâ–ˆâ–Œ       | 10/39 [03:31<10:12, 21.13s/it]Epoch 2/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:52<09:51, 21.12s/it]Epoch 2/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:52<09:51, 21.12s/it]Epoch 2/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:52<09:51, 21.12s/it]Epoch 2/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:52<09:51, 21.12s/it]step:49 - train/loss:0.110 - train/lr(1e-3):0.003Epoch 2/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:52<09:51, 21.12s/it]Epoch 2/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:52<09:51, 21.12s/it]
Epoch 2/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:52<09:51, 21.12s/it]Epoch 2/2:  28%|â–ˆâ–ˆâ–Š       | 11/39 [03:52<09:51, 21.12s/it]Epoch 2/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:14<09:32, 21.20s/it]Epoch 2/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:14<09:32, 21.20s/it]Epoch 2/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:14<09:32, 21.20s/it]Epoch 2/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:14<09:32, 21.20s/it]Epoch 2/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:14<09:32, 21.20s/it]Epoch 2/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:14<09:32, 21.20s/it]step:50 - train/loss:0.114 - train/lr(1e-3):0.003
Epoch 2/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:14<09:32, 21.20s/it]Epoch 2/2:  31%|â–ˆâ–ˆâ–ˆ       | 12/39 [04:14<09:32, 21.20s/it]Epoch 2/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:35<09:10, 21.17s/it]Epoch 2/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:35<09:10, 21.18s/it]Epoch 2/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:35<09:10, 21.17s/it]Epoch 2/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:35<09:10, 21.17s/it]step:51 - train/loss:0.107 - train/lr(1e-3):0.003Epoch 2/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:35<09:10, 21.17s/it]Epoch 2/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:35<09:10, 21.17s/it]
Epoch 2/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:35<09:10, 21.17s/it]Epoch 2/2:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 13/39 [04:35<09:10, 21.17s/it]Epoch 2/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [04:56<08:48, 21.15s/it]Epoch 2/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [04:56<08:48, 21.15s/it]Epoch 2/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [04:56<08:48, 21.15s/it]Epoch 2/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [04:56<08:48, 21.15s/it]Epoch 2/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [04:56<08:48, 21.15s/it]Epoch 2/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [04:56<08:48, 21.15s/it]step:52 - train/loss:0.109 - train/lr(1e-3):0.003
Epoch 2/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [04:56<08:48, 21.15s/it]Epoch 2/2:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/39 [04:56<08:48, 21.15s/it]Epoch 2/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:17<08:27, 21.15s/it]Epoch 2/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:17<08:27, 21.15s/it]Epoch 2/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:17<08:27, 21.15s/it]Epoch 2/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:17<08:27, 21.15s/it]Epoch 2/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:17<08:27, 21.15s/it]Epoch 2/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:17<08:27, 21.15s/it]step:53 - train/loss:0.107 - train/lr(1e-3):0.003Epoch 2/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:17<08:27, 21.15s/it]
Epoch 2/2:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/39 [05:17<08:27, 21.15s/it]Epoch 2/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:38<08:06, 21.14s/it]Epoch 2/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:38<08:06, 21.14s/it]Epoch 2/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:38<08:06, 21.14s/it]Epoch 2/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:38<08:06, 21.14s/it]Epoch 2/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:38<08:06, 21.14s/it]step:54 - train/loss:0.111 - train/lr(1e-3):0.002
Epoch 2/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:38<08:06, 21.14s/it]Epoch 2/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:38<08:06, 21.14s/it]Epoch 2/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/39 [05:38<08:06, 21.14s/it]Epoch 2/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [05:59<07:44, 21.13s/it]Epoch 2/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [05:59<07:44, 21.13s/it]Epoch 2/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [05:59<07:44, 21.13s/it]Epoch 2/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [05:59<07:44, 21.13s/it]Epoch 2/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [05:59<07:44, 21.13s/it]Epoch 2/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [05:59<07:44, 21.13s/it]step:55 - train/loss:0.106 - train/lr(1e-3):0.002Epoch 2/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [05:59<07:44, 21.13s/it]
Epoch 2/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 17/39 [05:59<07:44, 21.13s/it]Epoch 2/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:20<07:23, 21.12s/it]Epoch 2/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:20<07:23, 21.12s/it]Epoch 2/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:20<07:23, 21.12s/it]Epoch 2/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:20<07:23, 21.12s/it]Epoch 2/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:20<07:23, 21.12s/it]Epoch 2/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:20<07:23, 21.12s/it]step:56 - train/loss:0.106 - train/lr(1e-3):0.002
Epoch 2/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:20<07:23, 21.12s/it]Epoch 2/2:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/39 [06:20<07:23, 21.12s/it]Epoch 2/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:42<07:02, 21.11s/it]Epoch 2/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:42<07:02, 21.11s/it]Epoch 2/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:42<07:02, 21.11s/it]Epoch 2/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:42<07:02, 21.11s/it]Epoch 2/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:42<07:02, 21.11s/it]Epoch 2/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:42<07:02, 21.11s/it]step:57 - train/loss:0.109 - train/lr(1e-3):0.002
Epoch 2/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:42<07:02, 21.11s/it]Epoch 2/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/39 [06:42<07:02, 21.11s/it]Epoch 2/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:03<06:41, 21.12s/it]Epoch 2/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:03<06:41, 21.12s/it]Epoch 2/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:03<06:41, 21.12s/it]Epoch 2/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:03<06:41, 21.12s/it]Epoch 2/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:03<06:41, 21.12s/it]Epoch 2/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:03<06:41, 21.12s/it]step:58 - train/loss:0.110 - train/lr(1e-3):0.002
Epoch 2/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:03<06:41, 21.12s/it]Epoch 2/2:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/39 [07:03<06:41, 21.12s/it]Epoch 2/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:24<06:20, 21.12s/it]Epoch 2/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:24<06:20, 21.12s/it]Epoch 2/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:24<06:20, 21.12s/it]Epoch 2/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:24<06:20, 21.12s/it]Epoch 2/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:24<06:20, 21.12s/it]Epoch 2/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:24<06:20, 21.12s/it]step:59 - train/loss:0.111 - train/lr(1e-3):0.002
Epoch 2/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:24<06:20, 21.12s/it]Epoch 2/2:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/39 [07:24<06:20, 21.12s/it]Epoch 2/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:45<06:00, 21.21s/it]Epoch 2/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:45<06:00, 21.21s/it]Epoch 2/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:45<06:00, 21.21s/it]Epoch 2/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:45<06:00, 21.21s/it]Epoch 2/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:45<06:00, 21.21s/it]Epoch 2/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:45<06:00, 21.21s/it]step:60 - train/loss:0.111 - train/lr(1e-3):0.001
Epoch 2/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:45<06:00, 21.21s/it]Epoch 2/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 22/39 [07:45<06:00, 21.21s/it]Epoch 2/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:06<05:38, 21.18s/it]Epoch 2/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:06<05:38, 21.18s/it]Epoch 2/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:06<05:38, 21.18s/it]Epoch 2/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:06<05:38, 21.18s/it]Epoch 2/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:06<05:38, 21.18s/it]Epoch 2/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:06<05:38, 21.18s/it]step:61 - train/loss:0.106 - train/lr(1e-3):0.001
Epoch 2/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:06<05:38, 21.18s/it]Epoch 2/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 23/39 [08:06<05:38, 21.18s/it]Epoch 2/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:27<05:17, 21.16s/it]Epoch 2/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:27<05:17, 21.16s/it]Epoch 2/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:27<05:17, 21.16s/it]Epoch 2/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:27<05:17, 21.16s/it]Epoch 2/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:27<05:17, 21.16s/it]Epoch 2/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:27<05:17, 21.16s/it]step:62 - train/loss:0.108 - train/lr(1e-3):0.001
Epoch 2/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:27<05:17, 21.16s/it]Epoch 2/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/39 [08:27<05:17, 21.16s/it]Epoch 2/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:49<04:56, 21.15s/it]Epoch 2/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:49<04:56, 21.15s/it]Epoch 2/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:49<04:56, 21.15s/it]step:63 - train/loss:0.108 - train/lr(1e-3):0.001Epoch 2/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:49<04:56, 21.15s/it]Epoch 2/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:49<04:56, 21.15s/it]Epoch 2/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:49<04:56, 21.15s/it]
Epoch 2/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:49<04:56, 21.15s/it]Epoch 2/2:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/39 [08:49<04:56, 21.15s/it]Epoch 2/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:10<04:34, 21.13s/it]Epoch 2/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:10<04:34, 21.13s/it]Epoch 2/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:10<04:34, 21.13s/it]Epoch 2/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:10<04:34, 21.13s/it]Epoch 2/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:10<04:34, 21.13s/it]Epoch 2/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:10<04:34, 21.13s/it]step:64 - train/loss:0.111 - train/lr(1e-3):0.001Epoch 2/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:10<04:34, 21.13s/it]
Epoch 2/2:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 26/39 [09:10<04:34, 21.13s/it]Epoch 2/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:31<04:13, 21.12s/it]Epoch 2/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:31<04:13, 21.12s/it]Epoch 2/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:31<04:13, 21.12s/it]Epoch 2/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:31<04:13, 21.12s/it]Epoch 2/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:31<04:13, 21.12s/it]step:65 - train/loss:0.111 - train/lr(1e-3):0.001Epoch 2/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:31<04:13, 21.12s/it]
Epoch 2/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:31<04:13, 21.12s/it]Epoch 2/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 27/39 [09:31<04:13, 21.12s/it]Epoch 2/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:52<03:52, 21.12s/it]Epoch 2/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:52<03:52, 21.12s/it]Epoch 2/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:52<03:52, 21.12s/it]Epoch 2/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:52<03:52, 21.12s/it]step:66 - train/loss:0.109 - train/lr(1e-3):0.001Epoch 2/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:52<03:52, 21.12s/it]Epoch 2/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:52<03:52, 21.12s/it]
Epoch 2/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:52<03:52, 21.12s/it]Epoch 2/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 28/39 [09:52<03:52, 21.12s/it]Epoch 2/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:13<03:31, 21.12s/it]Epoch 2/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:13<03:31, 21.12s/it]Epoch 2/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:13<03:31, 21.12s/it]Epoch 2/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:13<03:31, 21.12s/it]Epoch 2/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:13<03:31, 21.12s/it]Epoch 2/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:13<03:31, 21.12s/it]step:67 - train/loss:0.108 - train/lr(1e-3):0.000
Epoch 2/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:13<03:31, 21.12s/it]Epoch 2/2:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 29/39 [10:13<03:31, 21.12s/it]Epoch 2/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:34<03:10, 21.12s/it]Epoch 2/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:34<03:10, 21.12s/it]Epoch 2/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:34<03:10, 21.12s/it]Epoch 2/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:34<03:10, 21.12s/it]Epoch 2/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:34<03:10, 21.12s/it]step:68 - train/loss:0.110 - train/lr(1e-3):0.000Epoch 2/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:34<03:10, 21.12s/it]
Epoch 2/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:34<03:10, 21.12s/it]Epoch 2/2:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 30/39 [10:34<03:10, 21.12s/it]Epoch 2/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:55<02:48, 21.11s/it]Epoch 2/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:55<02:48, 21.11s/it]Epoch 2/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:55<02:48, 21.11s/it]step:69 - train/loss:0.113 - train/lr(1e-3):0.000Epoch 2/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:55<02:48, 21.11s/it]Epoch 2/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:55<02:48, 21.11s/it]
Epoch 2/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:55<02:48, 21.11s/it]Epoch 2/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:55<02:48, 21.11s/it]Epoch 2/2:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 31/39 [10:55<02:48, 21.11s/it]Epoch 2/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:16<02:27, 21.12s/it]Epoch 2/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:16<02:27, 21.12s/it]Epoch 2/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:16<02:27, 21.12s/it]Epoch 2/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:16<02:27, 21.12s/it]Epoch 2/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:16<02:27, 21.12s/it]Epoch 2/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:16<02:27, 21.12s/it]step:70 - train/loss:0.109 - train/lr(1e-3):0.000Epoch 2/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:16<02:27, 21.12s/it]
Epoch 2/2:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 32/39 [11:16<02:27, 21.12s/it]Epoch 2/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:37<02:06, 21.12s/it]Epoch 2/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:37<02:06, 21.12s/it]step:71 - train/loss:0.106 - train/lr(1e-3):0.000Epoch 2/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:37<02:06, 21.12s/it]Epoch 2/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:37<02:06, 21.12s/it]Epoch 2/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:37<02:06, 21.12s/it]Epoch 2/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:37<02:06, 21.12s/it]Epoch 2/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:37<02:06, 21.12s/it]
Epoch 2/2:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 33/39 [11:37<02:06, 21.12s/it]Epoch 2/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [11:59<01:45, 21.12s/it]Epoch 2/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [11:59<01:45, 21.12s/it]step:72 - train/loss:0.110 - train/lr(1e-3):0.000Epoch 2/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [11:59<01:45, 21.12s/it]Epoch 2/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [11:59<01:45, 21.12s/it]Epoch 2/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [11:59<01:45, 21.12s/it]Epoch 2/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [11:59<01:45, 21.12s/it]
Epoch 2/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [11:59<01:45, 21.12s/it]Epoch 2/2:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 34/39 [11:59<01:45, 21.12s/it]Epoch 2/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:20<01:24, 21.12s/it]Epoch 2/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:20<01:24, 21.12s/it]Epoch 2/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:20<01:24, 21.12s/it]Epoch 2/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:20<01:24, 21.12s/it]Epoch 2/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:20<01:24, 21.12s/it]Epoch 2/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:20<01:24, 21.12s/it]Epoch 2/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:20<01:24, 21.12s/it]step:73 - train/loss:0.111 - train/lr(1e-3):0.000
Epoch 2/2:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 35/39 [12:20<01:24, 21.12s/it]Epoch 2/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:41<01:03, 21.12s/it]Epoch 2/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:41<01:03, 21.12s/it]step:74 - train/loss:0.112 - train/lr(1e-3):0.000Epoch 2/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:41<01:03, 21.12s/it]Epoch 2/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:41<01:03, 21.12s/it]Epoch 2/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:41<01:03, 21.12s/it]Epoch 2/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:41<01:03, 21.12s/it]Epoch 2/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:41<01:03, 21.12s/it]
Epoch 2/2:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 36/39 [12:41<01:03, 21.12s/it]Epoch 2/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:02<00:42, 21.12s/it]Epoch 2/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:02<00:42, 21.12s/it]Epoch 2/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:02<00:42, 21.11s/it]Epoch 2/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:02<00:42, 21.12s/it]Epoch 2/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:02<00:42, 21.12s/it]Epoch 2/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:02<00:42, 21.12s/it]Epoch 2/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:02<00:42, 21.12s/it]step:75 - train/loss:0.105 - train/lr(1e-3):0.000
Epoch 2/2:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 37/39 [13:02<00:42, 21.12s/it]Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:23<00:21, 21.12s/it]Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:23<00:21, 21.12s/it]Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:23<00:21, 21.12s/it]step:76 - train/loss:0.108 - train/lr(1e-3):0.000Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:23<00:21, 21.12s/it]Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:23<00:21, 21.12s/it]Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:23<00:21, 21.12s/it]
Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:23<00:21, 21.12s/it]Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [13:23<00:21, 21.12s/it]step:77 - train/loss:0.110 - train/lr(1e-3):0.000
step:78 - val/loss:0.109
/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [14:03<00:22, 22.19s/it]
Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [14:03<00:22, 22.19s/it]
Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [14:03<00:22, 22.19s/it]
Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [14:03<00:22, 22.19s/it]
Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [14:03<00:22, 22.19s/it]
Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [14:03<00:22, 22.19s/it]
Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [14:03<00:22, 22.19s/it]
Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 38/39 [14:03<00:22, 22.19s/it]
[rank1]:[W814 01:26:16.889653141 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank2]:[W814 01:26:16.899186097 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank3]:[W814 01:26:16.899397178 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank4]:[W814 01:26:16.903307921 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank7]:[W814 01:26:16.920613602 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank5]:[W814 01:26:16.994431546 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank6]:[W814 01:26:16.004145003 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     train/loss â–‡â–‡â–ˆâ–…â–„â–„â–…â–‚â–‚â–ƒâ–‚â–„â–ƒâ–…â–‚â–„â–ƒâ–„â–ƒâ–‚â–ƒâ–â–‚â–‚â–ƒâ–ƒâ–â–‚â–‚â–â–‚â–‚â–â–â–ƒâ–‚â–‚â–‚â–‚â–‚
wandb: train/lr(1e-3) â–‚â–ƒâ–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:       val/loss â–ˆâ–
wandb: 
wandb: Run summary:
wandb:     train/loss 0.10979
wandb: train/lr(1e-3) 0
wandb:       val/loss 0.1092
wandb: 
wandb: ðŸš€ View run 1.5b_pi1_sft_0814-0057 at: https://wandb.ai/coder66-RL-lab/sft/runs/wbfgzpqf
wandb: â­ï¸ View project at: https://wandb.ai/coder66-RL-lab/sft
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250814_005806-wbfgzpqf/logs
[rank0]:[W814 01:26:17.636689429 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
