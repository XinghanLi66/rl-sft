2025-07-30 11:32:56,791 INFO    MainThread:2523973 [wandb_setup.py:_flush():81] Current SDK version is 0.20.1
2025-07-30 11:32:56,791 INFO    MainThread:2523973 [wandb_setup.py:_flush():81] Configure stats pid to 2523973
2025-07-30 11:32:56,791 INFO    MainThread:2523973 [wandb_setup.py:_flush():81] Loading settings from /homes/gws/lxh22/.config/wandb/settings
2025-07-30 11:32:56,792 INFO    MainThread:2523973 [wandb_setup.py:_flush():81] Loading settings from /homes/gws/lxh22/rl-sft/rl-tests/wandb/settings
2025-07-30 11:32:56,792 INFO    MainThread:2523973 [wandb_setup.py:_flush():81] Loading settings from environment variables
2025-07-30 11:32:56,793 INFO    MainThread:2523973 [wandb_init.py:setup_run_log_directory():703] Logging user logs to /homes/gws/lxh22/rl-sft/rl-tests/wandb/run-20250730_113256-k478qxdn/logs/debug.log
2025-07-30 11:32:56,793 INFO    MainThread:2523973 [wandb_init.py:setup_run_log_directory():704] Logging internal logs to /homes/gws/lxh22/rl-sft/rl-tests/wandb/run-20250730_113256-k478qxdn/logs/debug-internal.log
2025-07-30 11:32:56,794 INFO    MainThread:2523973 [wandb_init.py:init():831] calling init triggers
2025-07-30 11:32:56,795 INFO    MainThread:2523973 [wandb_init.py:init():836] wandb.init called with sweep_config: {}
config: {'data': {'tokenizer': None, 'train_files': 'data/train/one_shot_rlvr/pi1_r128.parquet', 'val_files': 'data/test/math500.parquet', 'prompt_key': 'prompt', 'max_prompt_length': 1024, 'max_response_length': 3072, 'train_batch_size': 16, 'val_batch_size': 53, 'return_raw_input_ids': False, 'return_raw_chat': False, 'shuffle': True}, 'actor_rollout_ref': {'hybrid_engine': True, 'model': {'path': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B', 'external_lib': None, 'override_config': {}, 'enable_gradient_checkpointing': True, 'use_remove_padding': True, 'use_think': False}, 'actor': {'strategy': 'fsdp', 'ppo_mini_batch_size': 16, 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': None, 'use_dynamic_bsz': True, 'ppo_max_token_len_per_gpu': 24000, 'grad_clip': 1.0, 'clip_ratio': 0.2, 'entropy_coeff': 0.001, 'use_kl_loss': True, 'kl_loss_coef': 0.001, 'kl_loss_type': 'low_var_kl', 'ppo_epochs': 1, 'shuffle': False, 'ulysses_sequence_parallel_size': 1, 'optim': {'lr': 1e-06, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': 1600}, 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'optimizer_offload': False, 'fsdp_size': -1, 'grad_offload': False}}, 'ref': {'fsdp_config': {'param_offload': True, 'wrap_policy': {'min_num_params': 0}}, 'log_prob_micro_batch_size': None, 'log_prob_micro_batch_size_per_gpu': None, 'log_prob_use_dynamic_bsz': True, 'log_prob_max_token_len_per_gpu': 24000, 'ulysses_sequence_parallel_size': 1}, 'rollout': {'name': 'vllm', 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'prompt_length': 1024, 'response_length': 3072, 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.7, 'ignore_eos': False, 'enforce_eager': True, 'free_cache_engine': True, 'load_format': 'dummy_dtensor', 'tensor_model_parallel_size': 2, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'log_prob_micro_batch_size': None, 'log_prob_micro_batch_size_per_gpu': None, 'log_prob_use_dynamic_bsz': True, 'log_prob_max_token_len_per_gpu': 24000, 'disable_log_stats': True, 'enable_chunked_prefill': True, 'do_sample': True, 'n': 8, 'val_temperature': 0.6, 'n_val': 1}}, 'critic': {'strategy': 'fsdp', 'optim': {'lr': 1e-05, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': 1600}, 'model': {'path': '~/models/deepseek-llm-7b-chat', 'tokenizer_path': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B', 'override_config': {}, 'external_lib': None, 'enable_gradient_checkpointing': True, 'use_remove_padding': False, 'fsdp_config': {'param_offload': False, 'optimizer_offload': False, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': -1}}, 'ppo_mini_batch_size': 16, 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': None, 'forward_micro_batch_size': None, 'forward_micro_batch_size_per_gpu': None, 'use_dynamic_bsz': True, 'ppo_max_token_len_per_gpu': 32768, 'forward_max_token_len_per_gpu': 32768, 'ulysses_sequence_parallel_size': 1, 'ppo_epochs': 1, 'shuffle': False, 'grad_clip': 1.0, 'cliprange_value': 0.5}, 'reward_model': {'enable': False, 'strategy': 'fsdp', 'model': {'input_tokenizer': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B', 'path': '~/models/FsfairX-LLaMA3-RM-v0.1', 'external_lib': None, 'use_remove_padding': False, 'fsdp_config': {'min_num_params': 0, 'param_offload': False, 'fsdp_size': -1}}, 'micro_batch_size': None, 'micro_batch_size_per_gpu': None, 'max_length': None, 'ulysses_sequence_parallel_size': 1, 'use_dynamic_bsz': True, 'forward_max_token_len_per_gpu': 32768, 'reward_manager': 'naive'}, 'algorithm': {'gamma': 1.0, 'lam': 1.0, 'adv_estimator': 'grpo', 'kl_penalty': 'kl', 'kl_ctrl': {'type': 'fixed', 'kl_coef': 0.001}}, 'trainer': {'total_epochs': 200, 'total_training_steps': None, 'project_name': 'verl_few_shot', 'experiment_name': 'Qwen2.5-Math-1.5B-pi1_r128_nobalancebatch', 'logger': ['console', 'wandb'], 'val_generations_to_log_to_wandb': 0, 'nnodes': 1, 'n_gpus_per_node': 8, 'save_freq': 80, 'resume_mode': 'auto', 'resume_from_path': False, 'test_freq': 80, 'critic_warmup': 0, 'default_hdfs_dir': None, 'remove_previous_ckpt_in_save': False, 'del_local_ckpt_after_load': False, 'checkpoints_dir': '/local1/lxh/save', 'default_local_dir': '/local1/lxh/save/verl_few_shot/Qwen2.5-Math-1.5B-pi1_r128_nobalancebatch', 'val_before_train': True}, '_wandb': {}}
2025-07-30 11:32:56,796 INFO    MainThread:2523973 [wandb_init.py:init():872] starting backend
2025-07-30 11:32:57,023 INFO    MainThread:2523973 [wandb_init.py:init():875] sending inform_init request
2025-07-30 11:32:57,029 INFO    MainThread:2523973 [wandb_init.py:init():883] backend started and connected
2025-07-30 11:32:57,031 INFO    MainThread:2523973 [wandb_init.py:init():956] updated telemetry
2025-07-30 11:32:57,044 INFO    MainThread:2523973 [wandb_init.py:init():980] communicating run to backend with 90.0 second timeout
2025-07-30 11:32:57,376 INFO    MainThread:2523973 [wandb_init.py:init():1032] starting run threads in backend
2025-07-30 11:32:57,640 INFO    MainThread:2523973 [wandb_run.py:_console_start():2453] atexit reg
2025-07-30 11:32:57,640 INFO    MainThread:2523973 [wandb_run.py:_redirect():2301] redirect: wrap_raw
2025-07-30 11:32:57,641 INFO    MainThread:2523973 [wandb_run.py:_redirect():2370] Wrapping output streams.
2025-07-30 11:32:57,641 INFO    MainThread:2523973 [wandb_run.py:_redirect():2393] Redirects installed.
2025-07-30 11:32:57,647 INFO    MainThread:2523973 [wandb_init.py:init():1078] run started, returning control to user process
2025-07-30 16:38:15,236 INFO    MainThread:2523973 [wandb_run.py:_finish():2219] finishing run coder66-lab/verl_few_shot/k478qxdn
2025-07-30 16:38:15,237 INFO    MainThread:2523973 [wandb_run.py:_atexit_cleanup():2418] got exitcode: 0
2025-07-30 16:38:15,238 INFO    MainThread:2523973 [wandb_run.py:_restore():2400] restore
2025-07-30 16:38:15,238 INFO    MainThread:2523973 [wandb_run.py:_restore():2406] restore done
2025-07-30 16:38:15,744 INFO    MainThread:2523973 [wandb_run.py:_footer_history_summary_info():4000] rendering history
2025-07-30 16:38:15,746 INFO    MainThread:2523973 [wandb_run.py:_footer_history_summary_info():4032] rendering summary
2025-07-30 16:38:15,747 INFO    MainThread:2523973 [wandb_run.py:_footer_sync_info():3961] logging synced files
