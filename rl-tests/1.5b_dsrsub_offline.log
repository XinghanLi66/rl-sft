2025-08-08 01:00:30,118	INFO worker.py:1917 -- Started a local Ray instance.
[36m(main_task pid=2735423)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=2735423)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=2735423)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=2735423)[0m                                                  'grad_offload': False,
[36m(main_task pid=2735423)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=2735423)[0m                                                  'param_offload': False,
[36m(main_task pid=2735423)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=2735423)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=2735423)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=2735423)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=2735423)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=2735423)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=2735423)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=2735423)[0m                                            'total_training_steps': -1,
[36m(main_task pid=2735423)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=2735423)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=2735423)[0m                                  'ppo_max_token_len_per_gpu': 8192,
[36m(main_task pid=2735423)[0m                                  'ppo_micro_batch_size': None,
[36m(main_task pid=2735423)[0m                                  'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=2735423)[0m                                  'ppo_mini_batch_size': 128,
[36m(main_task pid=2735423)[0m                                  'shuffle': False,
[36m(main_task pid=2735423)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=2735423)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=2735423)[0m                                  'use_dynamic_bsz': True,
[36m(main_task pid=2735423)[0m                                  'use_kl_loss': True},
[36m(main_task pid=2735423)[0m                        'hybrid_engine': True,
[36m(main_task pid=2735423)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=2735423)[0m                                  'external_lib': None,
[36m(main_task pid=2735423)[0m                                  'override_config': {},
[36m(main_task pid=2735423)[0m                                  'path': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B',
[36m(main_task pid=2735423)[0m                                  'use_remove_padding': True,
[36m(main_task pid=2735423)[0m                                  'use_think': False},
[36m(main_task pid=2735423)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(main_task pid=2735423)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=2735423)[0m                                'log_prob_max_token_len_per_gpu': 8192,
[36m(main_task pid=2735423)[0m                                'log_prob_micro_batch_size': None,
[36m(main_task pid=2735423)[0m                                'log_prob_micro_batch_size_per_gpu': None,
[36m(main_task pid=2735423)[0m                                'log_prob_use_dynamic_bsz': True,
[36m(main_task pid=2735423)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=2735423)[0m                        'rollout': {'disable_log_stats': True,
[36m(main_task pid=2735423)[0m                                    'do_sample': True,
[36m(main_task pid=2735423)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=2735423)[0m                                    'enable_chunked_prefill': True,
[36m(main_task pid=2735423)[0m                                    'enforce_eager': True,
[36m(main_task pid=2735423)[0m                                    'free_cache_engine': True,
[36m(main_task pid=2735423)[0m                                    'gpu_memory_utilization': 0.6,
[36m(main_task pid=2735423)[0m                                    'ignore_eos': False,
[36m(main_task pid=2735423)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=2735423)[0m                                    'log_prob_max_token_len_per_gpu': 8192,
[36m(main_task pid=2735423)[0m                                    'log_prob_micro_batch_size': None,
[36m(main_task pid=2735423)[0m                                    'log_prob_micro_batch_size_per_gpu': None,
[36m(main_task pid=2735423)[0m                                    'log_prob_use_dynamic_bsz': True,
[36m(main_task pid=2735423)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=2735423)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=2735423)[0m                                    'n': 8,
[36m(main_task pid=2735423)[0m                                    'n_val': 1,
[36m(main_task pid=2735423)[0m                                    'name': 'vllm',
[36m(main_task pid=2735423)[0m                                    'prompt_length': 1024,
[36m(main_task pid=2735423)[0m                                    'response_length': 3072,
[36m(main_task pid=2735423)[0m                                    'temperature': 0.6,
[36m(main_task pid=2735423)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=2735423)[0m                                    'top_k': -1,
[36m(main_task pid=2735423)[0m                                    'top_p': 1,
[36m(main_task pid=2735423)[0m                                    'val_temperature': 0.6}},
[36m(main_task pid=2735423)[0m  'algorithm': {'adv_estimator': 'grpo_offline',
[36m(main_task pid=2735423)[0m                'gamma': 1.0,
[36m(main_task pid=2735423)[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},
[36m(main_task pid=2735423)[0m                'kl_penalty': 'kl',
[36m(main_task pid=2735423)[0m                'lam': 1.0},
[36m(main_task pid=2735423)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=2735423)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=2735423)[0m             'forward_micro_batch_size': None,
[36m(main_task pid=2735423)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(main_task pid=2735423)[0m             'grad_clip': 1.0,
[36m(main_task pid=2735423)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=2735423)[0m                       'external_lib': None,
[36m(main_task pid=2735423)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=2735423)[0m                                       'optimizer_offload': False,
[36m(main_task pid=2735423)[0m                                       'param_offload': False,
[36m(main_task pid=2735423)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=2735423)[0m                       'override_config': {},
[36m(main_task pid=2735423)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(main_task pid=2735423)[0m                       'tokenizer_path': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B',
[36m(main_task pid=2735423)[0m                       'use_remove_padding': False},
[36m(main_task pid=2735423)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=2735423)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=2735423)[0m                       'min_lr_ratio': None,
[36m(main_task pid=2735423)[0m                       'total_training_steps': -1,
[36m(main_task pid=2735423)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=2735423)[0m No module named 'vllm._version'
[36m(main_task pid=2735423)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=2736417)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=2736417)[0m No module named 'vllm._version'
[36m(pid=2736417)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=2736782)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=2736782)[0m No module named 'vllm._version'
[36m(pid=2736782)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=2736782)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2736782)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(pid=2736781)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=2736781)[0m No module named 'vllm._version'[32m [repeated 2x across cluster][0m
[36m(pid=2736781)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=2736417)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=2736417)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 2x across cluster][0m
[36m(main_task pid=2735423)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=2735423)[0m             'ppo_epochs': 1,
[36m(main_task pid=2735423)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=2735423)[0m             'ppo_micro_batch_size': None,
[36m(main_task pid=2735423)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=2735423)[0m             'ppo_mini_batch_size': 128,
[36m(main_task pid=2735423)[0m             'shuffle': False,
[36m(main_task pid=2735423)[0m             'strategy': 'fsdp',
[36m(main_task pid=2735423)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=2735423)[0m             'use_dynamic_bsz': True},
[36m(main_task pid=2735423)[0m  'data': {'max_prompt_length': 1024,
[36m(main_task pid=2735423)[0m           'max_response_length': 3072,
[36m(main_task pid=2735423)[0m           'prompt_key': 'prompt',
[36m(main_task pid=2735423)[0m           'return_raw_chat': False,
[36m(main_task pid=2735423)[0m           'return_raw_input_ids': False,
[36m(main_task pid=2735423)[0m           'shuffle': True,
[36m(main_task pid=2735423)[0m           'tokenizer': None,
[36m(main_task pid=2735423)[0m           'train_batch_size': 128,
[36m(main_task pid=2735423)[0m           'train_files': 'data/train/one_shot_rlvr/dsr_sub.parquet',
[36m(main_task pid=2735423)[0m           'val_batch_size': 530,
[36m(main_task pid=2735423)[0m           'val_files': 'data/test/math_minerva_aime25x8.parquet'},
[36m(main_task pid=2735423)[0m  'reward_model': {'enable': False,
[36m(main_task pid=2735423)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=2735423)[0m                   'max_length': None,
[36m(main_task pid=2735423)[0m                   'micro_batch_size': None,
[36m(main_task pid=2735423)[0m                   'micro_batch_size_per_gpu': None,
[36m(main_task pid=2735423)[0m                   'model': {'external_lib': None,
[36m(main_task pid=2735423)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=2735423)[0m                                             'min_num_params': 0,
[36m(main_task pid=2735423)[0m                                             'param_offload': False},
[36m(main_task pid=2735423)[0m                             'input_tokenizer': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B',
[36m(main_task pid=2735423)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=2735423)[0m                             'use_remove_padding': False},
[36m(main_task pid=2735423)[0m                   'reward_manager': 'naive',
[36m(main_task pid=2735423)[0m                   'strategy': 'fsdp',
[36m(main_task pid=2735423)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=2735423)[0m                   'use_dynamic_bsz': True},
[36m(main_task pid=2735423)[0m  'trainer': {'checkpoints_dir': '/local1/lxh/save',
[36m(main_task pid=2735423)[0m              'critic_warmup': 0,
[36m(main_task pid=2735423)[0m              'default_hdfs_dir': None,
[36m(main_task pid=2735423)[0m              'default_local_dir': '/local1/lxh/save/offline_grpo/Qwen2.5-Math-1.5B-dsr_sub_offline_0808',
[36m(main_task pid=2735423)[0m              'del_local_ckpt_after_load': False,
[36m(main_task pid=2735423)[0m              'experiment_name': 'Qwen2.5-Math-1.5B-dsr_sub_offline_0808',
[36m(main_task pid=2735423)[0m              'logger': ['console', 'wandb'],
[36m(main_task pid=2735423)[0m              'n_gpus_per_node': 4,
[36m(main_task pid=2735423)[0m              'nnodes': 1,
[36m(main_task pid=2735423)[0m              'project_name': 'offline_grpo',
[36m(main_task pid=2735423)[0m              'remove_previous_ckpt_in_save': False,
[36m(main_task pid=2735423)[0m              'resume_from_path': False,
[36m(main_task pid=2735423)[0m              'resume_mode': 'auto',
[36m(main_task pid=2735423)[0m              'save_freq': 20,
[36m(main_task pid=2735423)[0m              'test_freq': 20,
[36m(main_task pid=2735423)[0m              'total_epochs': 200,
[36m(main_task pid=2735423)[0m              'total_training_steps': None,
[36m(main_task pid=2735423)[0m              'val_before_train': True,
[36m(main_task pid=2735423)[0m              'val_generations_to_log_to_wandb': 0}}
[36m(main_task pid=2735423)[0m 
[36m(main_task pid=2735423)[0m Qwen or LLAMA---------------------------------
[36m(main_task pid=2735423)[0m 
[36m(main_task pid=2735423)[0m [validate_config] All configuration checks passed successfully!
[36m(main_task pid=2735423)[0m original dataset len: 1209
[36m(main_task pid=2735423)[0m filter dataset len: 1209
[36m(main_task pid=2735423)[0m Train_dataset size 1209
[36m(main_task pid=2735423)[0m original dataset len: 1012
[36m(main_task pid=2735423)[0m filter dataset len: 1012
[36m(main_task pid=2735423)[0m Size of train dataloader: 9
[36m(main_task pid=2735423)[0m Size of val dataloader: 1
[36m(main_task pid=2735423)[0m Total training steps: 1800
[36m(WorkerDict pid=2736417)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2736417)[0m   "architectures": [
[36m(WorkerDict pid=2736417)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2736417)[0m   ],
[36m(WorkerDict pid=2736417)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2736417)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=2736417)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2736417)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=2736417)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2736417)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=2736417)[0m   "max_position_embeddings": 4096,
[36m(WorkerDict pid=2736417)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=2736417)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2736417)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=2736417)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=2736417)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=2736417)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2736417)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2736417)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2736417)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=2736417)[0m   "sliding_window": 4096,
[36m(WorkerDict pid=2736417)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2736417)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2736417)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=2736417)[0m   "use_cache": true,
[36m(WorkerDict pid=2736417)[0m   "use_mrope": false,
[36m(WorkerDict pid=2736417)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2736417)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2736417)[0m }
[36m(WorkerDict pid=2736417)[0m 
[36m(WorkerDict pid=2736417)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=2736417)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=2736417)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f59a9aba050>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f59a9ab9f30>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=2736417)[0m ### weight_decay: 0.01
[36m(WorkerDict pid=2736417)[0m Total steps: 1800, num_warmup_steps: 0
[36m(WorkerDict pid=2736417)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2736417)[0m Before building vllm rollout, memory allocated (GB): 1.437696933746338, memory reserved (GB): 4.640625
[36m(WorkerDict pid=2736779)[0m INFO 08-08 01:01:33 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=2736779)[0m INFO 08-08 01:01:33 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=2736779)[0m WARNING 08-08 01:01:33 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=2736417)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=2736417)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=2736781)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2736781)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=2736417)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=2736782)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2736782)[0m   warnings.warn(
[36m(WorkerDict pid=2736779)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2736779)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f133437a050>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f1334379f30>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m ### weight_decay: 0.01[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m Total steps: 1800, num_warmup_steps: 0[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m Actor use_remove_padding=True[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736782)[0m local rank 0
[36m(WorkerDict pid=2736782)[0m INFO 08-08 01:01:33 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=2736417)[0m INFO 08-08 01:01:34 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=2736417)[0m INFO 08-08 01:01:34 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=2736781)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=2736417)[0m INFO 08-08 01:01:34 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f58a83699c0>, local_subscribe_port=56011, remote_subscribe_port=None)
[36m(WorkerDict pid=2736417)[0m before init cache memory allocated: 3.097089024GB, reserved: 3.252682752GB
[36m(WorkerDict pid=2736417)[0m after init cache memory allocated: 29.996471296GB, reserved: 30.205280256GB
[36m(WorkerDict pid=2736781)[0m INFO 08-08 01:01:33 config.py:887] Defaulting to use ray for distributed inference[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736781)[0m INFO 08-08 01:01:33 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736781)[0m WARNING 08-08 01:01:33 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736781)[0m local rank 0[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m INFO 08-08 01:01:34 selector.py:115] Using XFormers backend.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2736779)[0m INFO 08-08 01:01:34 utils.py:1008] Found nccl from library libnccl.so.2[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m INFO 08-08 01:01:34 pynccl.py:63] vLLM is using nccl==2.20.5[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736781)[0m INFO 08-08 01:01:34 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f821072dc00>, local_subscribe_port=37449, remote_subscribe_port=None)
[36m(WorkerDict pid=2736782)[0m kwargs: {'n': 8, 'logprobs': 1, 'max_tokens': 3072, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=2736417)[0m After building vllm rollout, memory allocated (GB): 26.498613357543945, memory reserved (GB): 28.130859375
[36m(WorkerDict pid=2736417)[0m After building sharding manager, memory allocated (GB): 26.498613357543945, memory reserved (GB): 28.130859375
[36m(WorkerDict pid=2736417)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2736417)[0m   "architectures": [
[36m(WorkerDict pid=2736417)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2736417)[0m   ],
[36m(WorkerDict pid=2736417)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2736417)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=2736417)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2736417)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=2736417)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2736417)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=2736417)[0m   "max_position_embeddings": 4096,
[36m(WorkerDict pid=2736417)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=2736417)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2736417)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=2736417)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=2736417)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=2736417)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2736417)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2736417)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2736417)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=2736417)[0m   "sliding_window": 4096,
[36m(WorkerDict pid=2736417)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2736417)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2736417)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=2736417)[0m   "use_cache": true,
[36m(WorkerDict pid=2736417)[0m   "use_mrope": false,
[36m(WorkerDict pid=2736417)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2736417)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2736417)[0m }
[36m(WorkerDict pid=2736417)[0m 
[36m(WorkerDict pid=2736417)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=2736417)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f59a9aba050>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f59a9ab9f30>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=2736417)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2736417)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2736417)[0m   "architectures": [
[36m(WorkerDict pid=2736417)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2736417)[0m   ],
[36m(WorkerDict pid=2736417)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2736417)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=2736417)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2736417)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=2736417)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2736417)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=2736417)[0m   "max_position_embeddings": 4096,
[36m(WorkerDict pid=2736417)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=2736417)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2736417)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=2736417)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=2736417)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=2736417)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2736417)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2736417)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2736417)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=2736417)[0m   "sliding_window": 4096,
[36m(WorkerDict pid=2736417)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2736417)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2736417)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=2736417)[0m   "use_cache": true,
[36m(WorkerDict pid=2736417)[0m   "use_mrope": false,
[36m(WorkerDict pid=2736417)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2736417)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2736417)[0m }
[36m(WorkerDict pid=2736417)[0m 
[36m(WorkerDict pid=2736417)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=2736417)[0m kwargs: {'n': 8, 'logprobs': 1, 'max_tokens': 3072, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736417)[0m ### weight_decay: 0.01
[36m(WorkerDict pid=2736417)[0m Total steps: 1800, num_warmup_steps: 0
[36m(WorkerDict pid=2736417)[0m Before building vllm rollout, memory allocated (GB): 27.93631076812744, memory reserved (GB): 31.21875
[36m(WorkerDict pid=2736417)[0m INFO 08-08 01:01:47 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=2736781)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=2736781)[0m   warnings.warn([32m [repeated 4x across cluster][0m
[36m(main_task pid=2735423)[0m wandb: Currently logged in as: lixinghan2013 (coder66-RL-lab) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=2735423)[0m wandb: Tracking run with wandb version 0.20.1
[36m(main_task pid=2735423)[0m wandb: Run data is saved locally in /homes/gws/lxh22/rl-sft/rl-tests/wandb/run-20250808_010154-sz2b5ooo
[36m(main_task pid=2735423)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=2735423)[0m wandb: Syncing run Qwen2.5-Math-1.5B-dsr_sub_offline_0808
[36m(main_task pid=2735423)[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/coder66-RL-lab/offline_grpo
[36m(main_task pid=2735423)[0m wandb: üöÄ View run at https://wandb.ai/coder66-RL-lab/offline_grpo/runs/sz2b5ooo
[36m(main_task pid=2735423)[0m wandb:                                                                                
[36m(main_task pid=2735423)[0m wandb: üöÄ View run Qwen2.5-Math-1.5B-dsr_sub_offline_0808 at: https://wandb.ai/coder66-RL-lab/offline_grpo/runs/sz2b5ooo
[36m(main_task pid=2735423)[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/coder66-RL-lab/offline_grpo
[36m(main_task pid=2735423)[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[36m(main_task pid=2735423)[0m wandb: Find logs at: ./wandb/run-20250808_010154-sz2b5ooo/logs
[36m(WorkerDict pid=2736417)[0m INFO 08-08 01:01:47 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=2736417)[0m WARNING 08-08 01:01:47 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=2736417)[0m local rank 0
[36m(WorkerDict pid=2736417)[0m before init cache memory allocated: 31.54202624GB, reserved: 31.778144256GB
[36m(WorkerDict pid=2736779)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f133437a050>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f1334379f30>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2736779)[0m Actor use_remove_padding=True[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2736781)[0m kwargs: {'n': 8, 'logprobs': 1, 'max_tokens': 3072, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=2736779)[0m ### weight_decay: 0.01[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m Total steps: 1800, num_warmup_steps: 0[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m INFO 08-08 01:01:47 config.py:887] Defaulting to use ray for distributed inference[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m INFO 08-08 01:01:47 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m WARNING 08-08 01:01:47 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736779)[0m local rank 0[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736782)[0m kwargs: {'n': 8, 'logprobs': 1, 'max_tokens': 3072, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=2736417)[0m after init cache memory allocated: 41.317343232GB, reserved: 41.584427008GB
[36m(WorkerDict pid=2736417)[0m After building vllm rollout, memory allocated (GB): 37.04028797149658, memory reserved (GB): 38.728515625
[36m(WorkerDict pid=2736417)[0m After building sharding manager, memory allocated (GB): 37.04028797149658, memory reserved (GB): 38.728515625
[36m(main_task pid=2735423)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(main_task pid=2735423)[0m Checkpoint tracker file does not exist: %s /local1/lxh/save/offline_grpo/Qwen2.5-Math-1.5B-dsr_sub_offline_0808/latest_checkpointed_iteration.txt
[36m(main_task pid=2735423)[0m Training from scratch
Error executing job with overrides: ['algorithm.adv_estimator=grpo_offline', 'data.train_files=data/train/one_shot_rlvr/dsr_sub.parquet', 'data.val_files=data/test/math_minerva_aime25x8.parquet', 'data.train_batch_size=128', 'data.val_batch_size=530', 'data.max_prompt_length=1024', 'data.max_response_length=3072', 'reward_model.reward_manager=naive', 'actor_rollout_ref.model.path=/homes/gws/lxh22/models/Qwen2.5-Math-1.5B', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=128', 'actor_rollout_ref.actor.use_dynamic_bsz=True', 'actor_rollout_ref.actor.ppo_max_token_len_per_gpu=8192', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', '+actor_rollout_ref.actor.fsdp_config.grad_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.temperature=0.6', '+actor_rollout_ref.rollout.val_temperature=0.6', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=8', '+actor_rollout_ref.rollout.n_val=1', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.critic_warmup=0', 'trainer.logger=[console,wandb]', 'trainer.project_name=offline_grpo', 'trainer.experiment_name=Qwen2.5-Math-1.5B-dsr_sub_offline_0808', 'trainer.checkpoints_dir=/local1/lxh/save', '+trainer.val_before_train=True', 'trainer.n_gpus_per_node=4', 'trainer.nnodes=1', 'trainer.save_freq=20', 'trainer.test_freq=20', 'trainer.default_hdfs_dir=null', 'trainer.total_epochs=200']
Traceback (most recent call last):
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/trainer/main_ppo_sft.py", line 24, in main
    run_ppo(config)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/trainer/main_ppo_sft.py", line 32, in run_ppo
    ray.get(main_task.remote(config, compute_score))
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/ray/_private/worker.py", line 2849, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/ray/_private/worker.py", line 937, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::main_task()[39m (pid=2735423, ip=10.158.48.113)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/trainer/main_ppo_sft.py", line 133, in main_task
    trainer.fit()
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/trainer/ppo/ray_trainer_sft.py", line 1108, in fit
    val_metrics = self._validate()
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/trainer/ppo/ray_trainer_sft.py", line 716, in _validate
    test_output_gen_batch_padded = self.actor_rollout_wg.generate_sequences(test_gen_batch_padded)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=2736782, ip=10.158.48.113, actor_id=419cece4ab09c0fdc26cfed701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7ed8fc68ef50>)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/workers/fsdp_workers.py", line 524, in generate_sequences
    with self.rollout_sharding_manager:
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/workers/sharding_manager/fsdp_vllm.py", line 72, in __enter__
    params = self.module.state_dict()
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1941, in state_dict
    hook_result = hook(self, destination, prefix, local_metadata)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 729, in _post_state_dict_hook
    processed_state_dict = _post_state_dict_hook_fn[fsdp_state._state_dict_type](
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 574, in _sharded_post_state_dict_hook
    return _common_unshard_post_state_dict_hook(
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 243, in _common_unshard_post_state_dict_hook
    param_hook(state_dict, prefix, fqn)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py", line 564, in param_hook
    sharded_tensor = _ext_chunk_dtensor(
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/_fsdp_extensions.py", line 150, in _ext_chunk_dtensor
    return chunk_dtensor_fn(
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/_shard_utils.py", line 107, in _create_chunk_dtensor
    return DTensor.from_local(
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 483, in redistribute
    return Redistribute.apply(self, device_mesh, placements, async_op)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/autograd/function.py", line 574, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/_tensor/_redistribute.py", line 282, in forward
    output = redistribute_local_tensor(
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/_tensor/_redistribute.py", line 206, in redistribute_local_tensor
    new_local_tensor = target_placement._replicate_to_shard(
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/_tensor/placement_types.py", line 262, in _replicate_to_shard
    return shards[shard_index].clone()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 47.41 GiB of which 156.06 MiB is free. Process 1932665 has 35.72 GiB memory in use. Including non-PyTorch memory, this process has 11.50 GiB memory in use. Of the allocated memory 10.27 GiB is allocated by PyTorch, and 344.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(WorkerDict pid=2736417)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736417)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2736417)[0m kwargs: {'n': 8, 'logprobs': 1, 'max_tokens': 3072, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}[32m [repeated 2x across cluster][0m
