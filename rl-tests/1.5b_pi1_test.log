2025-08-13 16:15:50,721	INFO worker.py:1917 -- Started a local Ray instance.
[36m(main_task pid=1069846)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=1069846)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=1069846)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=1069846)[0m                                                  'grad_offload': False,
[36m(main_task pid=1069846)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=1069846)[0m                                                  'param_offload': False,
[36m(main_task pid=1069846)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=1069846)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=1069846)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=1069846)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=1069846)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=1069846)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=1069846)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=1069846)[0m                                            'total_training_steps': -1,
[36m(main_task pid=1069846)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=1069846)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=1069846)[0m                                  'ppo_max_token_len_per_gpu': 8192,
[36m(main_task pid=1069846)[0m                                  'ppo_micro_batch_size': None,
[36m(main_task pid=1069846)[0m                                  'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=1069846)[0m                                  'ppo_mini_batch_size': 16,
[36m(main_task pid=1069846)[0m                                  'shuffle': False,
[36m(main_task pid=1069846)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=1069846)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=1069846)[0m                                  'use_dynamic_bsz': True,
[36m(main_task pid=1069846)[0m                                  'use_kl_loss': True},
[36m(main_task pid=1069846)[0m                        'hybrid_engine': True,
[36m(main_task pid=1069846)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=1069846)[0m                                  'external_lib': None,
[36m(main_task pid=1069846)[0m                                  'override_config': {},
[36m(main_task pid=1069846)[0m                                  'path': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B',
[36m(main_task pid=1069846)[0m                                  'use_remove_padding': True,
[36m(main_task pid=1069846)[0m                                  'use_think': False},
[36m(main_task pid=1069846)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(main_task pid=1069846)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=1069846)[0m                                'log_prob_max_token_len_per_gpu': 8192,
[36m(main_task pid=1069846)[0m                                'log_prob_micro_batch_size': None,
[36m(main_task pid=1069846)[0m                                'log_prob_micro_batch_size_per_gpu': None,
[36m(main_task pid=1069846)[0m                                'log_prob_use_dynamic_bsz': True,
[36m(main_task pid=1069846)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=1069846)[0m                        'rollout': {'disable_log_stats': True,
[36m(main_task pid=1069846)[0m                                    'do_sample': True,
[36m(main_task pid=1069846)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=1069846)[0m                                    'enable_chunked_prefill': True,
[36m(main_task pid=1069846)[0m                                    'enforce_eager': True,
[36m(main_task pid=1069846)[0m                                    'free_cache_engine': True,
[36m(main_task pid=1069846)[0m                                    'gpu_memory_utilization': 0.6,
[36m(main_task pid=1069846)[0m                                    'ignore_eos': False,
[36m(main_task pid=1069846)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=1069846)[0m                                    'log_prob_max_token_len_per_gpu': 8192,
[36m(main_task pid=1069846)[0m                                    'log_prob_micro_batch_size': None,
[36m(main_task pid=1069846)[0m                                    'log_prob_micro_batch_size_per_gpu': None,
[36m(main_task pid=1069846)[0m                                    'log_prob_use_dynamic_bsz': True,
[36m(main_task pid=1069846)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=1069846)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=1069846)[0m                                    'n': 4,
[36m(main_task pid=1069846)[0m                                    'n_val': 1,
[36m(main_task pid=1069846)[0m                                    'name': 'vllm',
[36m(main_task pid=1069846)[0m                                    'prompt_length': 1024,
[36m(main_task pid=1069846)[0m                                    'response_length': 3072,
[36m(main_task pid=1069846)[0m                                    'temperature': 0.6,
[36m(main_task pid=1069846)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=1069846)[0m                                    'top_k': -1,
[36m(main_task pid=1069846)[0m                                    'top_p': 1,
[36m(main_task pid=1069846)[0m                                    'val_temperature': 0.6}},
[36m(main_task pid=1069846)[0m  'algorithm': {'adv_estimator': 'grpo_offline',
[36m(main_task pid=1069846)[0m                'gamma': 1.0,
[36m(main_task pid=1069846)[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},
[36m(main_task pid=1069846)[0m                'kl_penalty': 'kl',
[36m(main_task pid=1069846)[0m                'lam': 1.0},
[36m(main_task pid=1069846)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=1069846)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=1069846)[0m             'forward_micro_batch_size': None,
[36m(main_task pid=1069846)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(main_task pid=1069846)[0m             'grad_clip': 1.0,
[36m(main_task pid=1069846)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=1069846)[0m                       'external_lib': None,
[36m(main_task pid=1069846)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=1069846)[0m                                       'optimizer_offload': False,
[36m(main_task pid=1069846)[0m                                       'param_offload': False,
[36m(main_task pid=1069846)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=1069846)[0m                       'override_config': {},
[36m(main_task pid=1069846)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(main_task pid=1069846)[0m                       'tokenizer_path': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B',
[36m(main_task pid=1069846)[0m                       'use_remove_padding': False},
[36m(main_task pid=1069846)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=1069846)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=1069846)[0m                       'min_lr_ratio': None,
[36m(main_task pid=1069846)[0m                       'total_training_steps': -1,
[36m(main_task pid=1069846)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=1069846)[0m No module named 'vllm._version'
[36m(main_task pid=1069846)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=1070690)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=1070690)[0m No module named 'vllm._version'
[36m(pid=1070690)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=1071018)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=1071018)[0m No module named 'vllm._version'
[36m(pid=1071018)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=1070690)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=1070690)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=1071018)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=1071018)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(main_task pid=1069846)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=1069846)[0m             'ppo_epochs': 1,
[36m(main_task pid=1069846)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=1069846)[0m             'ppo_micro_batch_size': None,
[36m(main_task pid=1069846)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=1069846)[0m             'ppo_mini_batch_size': 16,
[36m(main_task pid=1069846)[0m             'shuffle': False,
[36m(main_task pid=1069846)[0m             'strategy': 'fsdp',
[36m(main_task pid=1069846)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=1069846)[0m             'use_dynamic_bsz': True},
[36m(main_task pid=1069846)[0m  'data': {'max_prompt_length': 1024,
[36m(main_task pid=1069846)[0m           'max_response_length': 3072,
[36m(main_task pid=1069846)[0m           'prompt_key': 'prompt',
[36m(main_task pid=1069846)[0m           'return_raw_chat': False,
[36m(main_task pid=1069846)[0m           'return_raw_input_ids': False,
[36m(main_task pid=1069846)[0m           'shuffle': True,
[36m(main_task pid=1069846)[0m           'tokenizer': None,
[36m(main_task pid=1069846)[0m           'train_batch_size': 16,
[36m(main_task pid=1069846)[0m           'train_files': 'data/train/one_shot_rlvr/pi1_r128.parquet',
[36m(main_task pid=1069846)[0m           'val_batch_size': 53,
[36m(main_task pid=1069846)[0m           'val_files': 'data/test/math_minerva_aime25x8.parquet'},
[36m(main_task pid=1069846)[0m  'reward_model': {'enable': False,
[36m(main_task pid=1069846)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=1069846)[0m                   'max_length': None,
[36m(main_task pid=1069846)[0m                   'micro_batch_size': None,
[36m(main_task pid=1069846)[0m                   'micro_batch_size_per_gpu': None,
[36m(main_task pid=1069846)[0m                   'model': {'external_lib': None,
[36m(main_task pid=1069846)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=1069846)[0m                                             'min_num_params': 0,
[36m(main_task pid=1069846)[0m                                             'param_offload': False},
[36m(main_task pid=1069846)[0m                             'input_tokenizer': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B',
[36m(main_task pid=1069846)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=1069846)[0m                             'use_remove_padding': False},
[36m(main_task pid=1069846)[0m                   'reward_manager': 'naive',
[36m(main_task pid=1069846)[0m                   'strategy': 'fsdp',
[36m(main_task pid=1069846)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=1069846)[0m                   'use_dynamic_bsz': True},
[36m(main_task pid=1069846)[0m  'trainer': {'checkpoints_dir': '/local1/lxh/save',
[36m(main_task pid=1069846)[0m              'critic_warmup': 0,
[36m(main_task pid=1069846)[0m              'default_hdfs_dir': None,
[36m(main_task pid=1069846)[0m              'default_local_dir': '/local1/lxh/save/offline_grpo/Qwen2.5-Math-1.5B-pi1_r128_test_0813',
[36m(main_task pid=1069846)[0m              'del_local_ckpt_after_load': False,
[36m(main_task pid=1069846)[0m              'experiment_name': 'Qwen2.5-Math-1.5B-pi1_r128_test_0813',
[36m(main_task pid=1069846)[0m              'logger': ['console', 'wandb'],
[36m(main_task pid=1069846)[0m              'n_gpus_per_node': 2,
[36m(main_task pid=1069846)[0m              'nnodes': 1,
[36m(main_task pid=1069846)[0m              'project_name': 'offline_grpo',
[36m(main_task pid=1069846)[0m              'remove_previous_ckpt_in_save': False,
[36m(main_task pid=1069846)[0m              'resume_from_path': False,
[36m(main_task pid=1069846)[0m              'resume_mode': 'auto',
[36m(main_task pid=1069846)[0m              'save_freq': -1,
[36m(main_task pid=1069846)[0m              'test_freq': 5,
[36m(main_task pid=1069846)[0m              'total_epochs': 200,
[36m(main_task pid=1069846)[0m              'total_training_steps': None,
[36m(main_task pid=1069846)[0m              'val_before_train': True,
[36m(main_task pid=1069846)[0m              'val_generations_to_log_to_wandb': 0}}
[36m(main_task pid=1069846)[0m 
[36m(main_task pid=1069846)[0m Qwen or LLAMA---------------------------------
[36m(main_task pid=1069846)[0m 
[36m(main_task pid=1069846)[0m [validate_config] All configuration checks passed successfully!
[36m(main_task pid=1069846)[0m original dataset len: 128
[36m(main_task pid=1069846)[0m filter dataset len: 128
[36m(main_task pid=1069846)[0m Train_dataset size 128
[36m(main_task pid=1069846)[0m original dataset len: 1012
[36m(main_task pid=1069846)[0m filter dataset len: 1012
[36m(main_task pid=1069846)[0m Size of train dataloader: 8
[36m(main_task pid=1069846)[0m Size of val dataloader: 1
[36m(main_task pid=1069846)[0m Total training steps: 1600
[36m(WorkerDict pid=1070690)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=1070690)[0m   "architectures": [
[36m(WorkerDict pid=1070690)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=1070690)[0m   ],
[36m(WorkerDict pid=1070690)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=1070690)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=1070690)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=1070690)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=1070690)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=1070690)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=1070690)[0m   "max_position_embeddings": 4096,
[36m(WorkerDict pid=1070690)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=1070690)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=1070690)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=1070690)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=1070690)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=1070690)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=1070690)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=1070690)[0m   "rope_scaling": null,
[36m(WorkerDict pid=1070690)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=1070690)[0m   "sliding_window": 4096,
[36m(WorkerDict pid=1070690)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=1070690)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=1070690)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=1070690)[0m   "use_cache": true,
[36m(WorkerDict pid=1070690)[0m   "use_mrope": false,
[36m(WorkerDict pid=1070690)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=1070690)[0m   "vocab_size": 151936
[36m(WorkerDict pid=1070690)[0m }
[36m(WorkerDict pid=1070690)[0m 
[36m(WorkerDict pid=1070690)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=1070690)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=1070690)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fa90aa8e050>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fa90aa8df30>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=1070690)[0m ### weight_decay: 0.01
[36m(WorkerDict pid=1070690)[0m Total steps: 1600, num_warmup_steps: 0
[36m(WorkerDict pid=1070690)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=1070690)[0m Before building vllm rollout, memory allocated (GB): 2.8964853286743164, memory reserved (GB): 6.060546875
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:16:53 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:16:53 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=1070690)[0m WARNING 08-13 16:16:53 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=1070690)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=1070690)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=1070690)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=1070690)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=1070690)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=1070690)[0m   warnings.warn(
[36m(WorkerDict pid=1071018)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=1071018)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=1071018)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=1071018)[0m wrap_policy: functools.partial(<function _or_policy at 0x7efd25a02050>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7efd25a01f30>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=1071018)[0m ### weight_decay: 0.01
[36m(WorkerDict pid=1071018)[0m Total steps: 1600, num_warmup_steps: 0
[36m(WorkerDict pid=1071018)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=1070690)[0m local rank 0
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:16:54 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:16:57 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:16:57 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:17:02 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fa80419cd30>, local_subscribe_port=58601, remote_subscribe_port=None)
[36m(WorkerDict pid=1071018)[0m INFO 08-13 16:16:57 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=1071018)[0m INFO 08-13 16:16:57 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=1071018)[0m WARNING 08-13 16:16:57 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=1071018)[0m local rank 0
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:17:02 selector.py:115] Using XFormers backend.[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=1070690)[0m before init cache memory allocated: 4.663451136GB, reserved: 4.777312256GB
[36m(WorkerDict pid=1071018)[0m INFO 08-13 16:16:57 utils.py:1008] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=1071018)[0m INFO 08-13 16:16:57 pynccl.py:63] vLLM is using nccl==2.20.5
[36m(WorkerDict pid=1071018)[0m INFO 08-13 16:17:02 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=1070690)[0m after init cache memory allocated: 8.069225984GB, reserved: 8.183087104GB
[36m(WorkerDict pid=1070690)[0m kwargs: {'n': 4, 'logprobs': 1, 'max_tokens': 3072, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=1070690)[0m After building vllm rollout, memory allocated (GB): 6.077274799346924, memory reserved (GB): 7.62109375
[36m(WorkerDict pid=1070690)[0m After building sharding manager, memory allocated (GB): 6.077274799346924, memory reserved (GB): 7.62109375
[36m(WorkerDict pid=1070690)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=1070690)[0m   "architectures": [
[36m(WorkerDict pid=1070690)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=1070690)[0m   ],
[36m(WorkerDict pid=1070690)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=1070690)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=1070690)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=1070690)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=1070690)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=1070690)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=1070690)[0m   "max_position_embeddings": 4096,
[36m(WorkerDict pid=1070690)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=1070690)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=1070690)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=1070690)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=1070690)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=1070690)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=1070690)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=1070690)[0m   "rope_scaling": null,
[36m(WorkerDict pid=1070690)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=1070690)[0m   "sliding_window": 4096,
[36m(WorkerDict pid=1070690)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=1070690)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=1070690)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=1070690)[0m   "use_cache": true,
[36m(WorkerDict pid=1070690)[0m   "use_mrope": false,
[36m(WorkerDict pid=1070690)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=1070690)[0m   "vocab_size": 151936
[36m(WorkerDict pid=1070690)[0m }
[36m(WorkerDict pid=1070690)[0m 
[36m(WorkerDict pid=1070690)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=1070690)[0m wrap_policy: functools.partial(<function _or_policy at 0x7fa90aa8e050>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7fa90aa8df30>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=1070690)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=1070690)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=1070690)[0m   "architectures": [
[36m(WorkerDict pid=1070690)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=1070690)[0m   ],
[36m(WorkerDict pid=1070690)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=1070690)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=1070690)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=1070690)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=1070690)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=1070690)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=1070690)[0m   "max_position_embeddings": 4096,
[36m(WorkerDict pid=1070690)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=1070690)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=1070690)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=1070690)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=1070690)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=1070690)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=1070690)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=1070690)[0m   "rope_scaling": null,
[36m(WorkerDict pid=1070690)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=1070690)[0m   "sliding_window": 4096,
[36m(WorkerDict pid=1070690)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=1070690)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=1070690)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=1070690)[0m   "use_cache": true,
[36m(WorkerDict pid=1070690)[0m   "use_mrope": false,
[36m(WorkerDict pid=1070690)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=1070690)[0m   "vocab_size": 151936
[36m(WorkerDict pid=1070690)[0m }
[36m(WorkerDict pid=1070690)[0m 
[36m(WorkerDict pid=1070690)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=1071018)[0m kwargs: {'n': 4, 'logprobs': 1, 'max_tokens': 3072, 'detokenize': False, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=1070690)[0m ### weight_decay: 0.01
[36m(WorkerDict pid=1070690)[0m Total steps: 1600, num_warmup_steps: 0
[36m(WorkerDict pid=1070690)[0m Before building vllm rollout, memory allocated (GB): 8.973760604858398, memory reserved (GB): 10.728515625
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:17:19 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:17:19 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=1070690)[0m WARNING 08-13 16:17:19 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=1070690)[0m local rank 0
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:17:20 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250813-161720.pkl...
[36m(main_task pid=1069846)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=1071018, ip=172.28.6.60, actor_id=1c3bbcca365ae0f0e717c0ed01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7efcfec46980>)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1708, in execute_model
[36m(main_task pid=1069846)[0m     output: SamplerOutput = self.model.sample(
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 433, in sample
[36m(main_task pid=1069846)[0m     next_tokens = self.sampler(logits, sampling_metadata)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[36m(main_task pid=1069846)[0m     return self._call_impl(*args, **kwargs)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[36m(main_task pid=1069846)[0m     return forward_call(*args, **kwargs)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 261, in forward
[36m(main_task pid=1069846)[0m     logits = _apply_top_k_top_p(logits, sampling_tensors.top_ps,
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 419, in _apply_top_k_top_p
[36m(main_task pid=1069846)[0m     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[36m(main_task pid=1069846)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 44.43 GiB of which 248.62 MiB is free. Process 1051395 has 32.22 GiB memory in use. Including non-PyTorch memory, this process has 11.91 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 100.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(main_task pid=1069846)[0m 
[36m(main_task pid=1069846)[0m The above exception was the direct cause of the following exception:
[36m(main_task pid=1069846)[0m 
[36m(main_task pid=1069846)[0m [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=1071018, ip=172.28.6.60, actor_id=1c3bbcca365ae0f0e717c0ed01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7efcfec46980>)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/rl-sft/rl-tests/verl/single_controller/ray/base.py", line 399, in func
[36m(main_task pid=1069846)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/rl-sft/rl-tests/verl/single_controller/base/decorator.py", line 404, in inner
[36m(main_task pid=1069846)[0m     return func(*args, **kwargs)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/rl-sft/rl-tests/verl/workers/fsdp_workers.py", line 381, in init_model
[36m(main_task pid=1069846)[0m     self.rollout, self.rollout_sharding_manager = self._build_rollout()
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/rl-sft/rl-tests/verl/workers/fsdp_workers.py", line 311, in _build_rollout
[36m(main_task pid=1069846)[0m     rollout = vLLMRollout(actor_module=self.actor_module_fsdp,
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/rl-sft/rl-tests/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 92, in __init__
[36m(main_task pid=1069846)[0m     self.inference_engine = LLM(
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/rl-sft/rl-tests/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 142, in __init__
[36m(main_task pid=1069846)[0m     self.llm_engine = LLMEngine.from_engine_args(model, tokenizer, engine_args)  # TODO: check usagecontext
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/rl-sft/rl-tests/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 393, in from_engine_args
[36m(main_task pid=1069846)[0m     engine = cls(
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/rl-sft/rl-tests/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 227, in __init__
[36m(main_task pid=1069846)[0m     self._initialize_kv_caches()
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 484, in _initialize_kv_caches
[36m(main_task pid=1069846)[0m     self.model_executor.determine_num_available_blocks())
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/rl-sft/rl-tests/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 125, in determine_num_available_blocks
[36m(main_task pid=1069846)[0m     num_blocks = self.worker.determine_num_available_blocks()
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[36m(main_task pid=1069846)[0m     return func(*args, **kwargs)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/rl-sft/rl-tests/verl/third_party/vllm/vllm_v_0_6_3/worker.py", line 196, in determine_num_available_blocks
[36m(main_task pid=1069846)[0m     self.model_runner.profile_run()
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[36m(main_task pid=1069846)[0m     return func(*args, **kwargs)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1309, in profile_run
[36m(main_task pid=1069846)[0m     self.execute_model(model_input, kv_caches, intermediate_tensors)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[36m(main_task pid=1069846)[0m     return func(*args, **kwargs)
[36m(main_task pid=1069846)[0m   File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 152, in _wrapper
[36m(main_task pid=1069846)[0m     raise type(err)(
[36m(main_task pid=1069846)[0m torch.OutOfMemoryError: Error in model execution (input dumped to /tmp/err_execute_model_input_20250813-161720.pkl): CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 44.43 GiB of which 248.62 MiB is free. Process 1051395 has 32.22 GiB memory in use. Including non-PyTorch memory, this process has 11.91 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 100.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(WorkerDict pid=1071018)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=1071018)[0m   warnings.warn(
[36m(WorkerDict pid=1070690)[0m INFO 08-13 16:17:20 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20250813-161720.pkl.
[36m(WorkerDict pid=1071018)[0m wrap_policy: functools.partial(<function _or_policy at 0x7efd25a02050>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7efd25a01f30>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 3x across cluster][0m
Error executing job with overrides: ['algorithm.adv_estimator=grpo_offline', 'data.train_files=data/train/one_shot_rlvr/pi1_r128.parquet', 'data.val_files=data/test/math_minerva_aime25x8.parquet', 'data.train_batch_size=16', 'data.val_batch_size=53', 'data.max_prompt_length=1024', 'data.max_response_length=3072', 'reward_model.reward_manager=naive', 'actor_rollout_ref.model.path=/homes/gws/lxh22/models/Qwen2.5-Math-1.5B', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=16', 'actor_rollout_ref.actor.use_dynamic_bsz=True', 'actor_rollout_ref.actor.ppo_max_token_len_per_gpu=8192', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', '+actor_rollout_ref.actor.fsdp_config.grad_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.tensor_model_parallel_size=2', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.temperature=0.6', '+actor_rollout_ref.rollout.val_temperature=0.6', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.6', 'actor_rollout_ref.rollout.n=4', '+actor_rollout_ref.rollout.n_val=1', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.critic_warmup=0', 'trainer.logger=[console,wandb]', 'trainer.project_name=offline_grpo', 'trainer.experiment_name=Qwen2.5-Math-1.5B-pi1_r128_test_0813', 'trainer.checkpoints_dir=/local1/lxh/save', '+trainer.val_before_train=True', 'trainer.n_gpus_per_node=2', 'trainer.nnodes=1', 'trainer.save_freq=-1', 'trainer.test_freq=5', 'trainer.default_hdfs_dir=null', 'trainer.total_epochs=200']
Traceback (most recent call last):
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/trainer/main_ppo_sft.py", line 24, in main
    run_ppo(config)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/trainer/main_ppo_sft.py", line 32, in run_ppo
    ray.get(main_task.remote(config, compute_score))
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/ray/_private/worker.py", line 2849, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/ray/_private/worker.py", line 937, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::main_task()[39m (pid=1069846, ip=172.28.6.60)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/trainer/main_ppo_sft.py", line 132, in main_task
    trainer.init_workers()
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/trainer/ppo/ray_trainer_sft.py", line 840, in init_workers
    self.actor_rollout_wg.init_model()
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_init_model()[39m (pid=1070690, ip=172.28.6.60, actor_id=71213c3b2f1614bcd49e90aa01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fa8e324e980>)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1708, in execute_model
    output: SamplerOutput = self.model.sample(
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py", line 433, in sample
    next_tokens = self.sampler(logits, sampling_metadata)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 261, in forward
    logits = _apply_top_k_top_p(logits, sampling_tensors.top_ps,
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py", line 419, in _apply_top_k_top_p
    logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 44.43 GiB of which 248.62 MiB is free. Process 1051394 has 32.22 GiB memory in use. Including non-PyTorch memory, this process has 11.91 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 100.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

The above exception was the direct cause of the following exception:

[36mray::WorkerDict.actor_rollout_init_model()[39m (pid=1070690, ip=172.28.6.60, actor_id=71213c3b2f1614bcd49e90aa01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fa8e324e980>)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/workers/fsdp_workers.py", line 381, in init_model
    self.rollout, self.rollout_sharding_manager = self._build_rollout()
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/workers/fsdp_workers.py", line 311, in _build_rollout
    rollout = vLLMRollout(actor_module=self.actor_module_fsdp,
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/workers/rollout/vllm_rollout/vllm_rollout.py", line 92, in __init__
    self.inference_engine = LLM(
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/third_party/vllm/vllm_v_0_6_3/llm.py", line 142, in __init__
    self.llm_engine = LLMEngine.from_engine_args(model, tokenizer, engine_args)  # TODO: check usagecontext
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 393, in from_engine_args
    engine = cls(
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/third_party/vllm/vllm_v_0_6_3/llm_engine_sp.py", line 227, in __init__
    self._initialize_kv_caches()
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 484, in _initialize_kv_caches
    self.model_executor.determine_num_available_blocks())
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/third_party/vllm/vllm_v_0_6_3/spmd_gpu_executor.py", line 125, in determine_num_available_blocks
    num_blocks = self.worker.determine_num_available_blocks()
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/homes/gws/lxh22/rl-sft/rl-tests/verl/third_party/vllm/vllm_v_0_6_3/worker.py", line 196, in determine_num_available_blocks
    self.model_runner.profile_run()
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 1309, in profile_run
    self.execute_model(model_input, kv_caches, intermediate_tensors)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/worker/model_runner_base.py", line 152, in _wrapper
    raise type(err)(
torch.OutOfMemoryError: Error in model execution (input dumped to /tmp/err_execute_model_input_20250813-161720.pkl): CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 44.43 GiB of which 248.62 MiB is free. Process 1051394 has 32.22 GiB memory in use. Including non-PyTorch memory, this process has 11.91 GiB memory in use. Of the allocated memory 11.09 GiB is allocated by PyTorch, and 100.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[36m(WorkerDict pid=1071018)[0m Actor use_remove_padding=True[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=1071018)[0m ### weight_decay: 0.01
[36m(WorkerDict pid=1071018)[0m Total steps: 1600, num_warmup_steps: 0
[36m(WorkerDict pid=1071018)[0m INFO 08-13 16:17:19 config.py:887] Defaulting to use ray for distributed inference
[36m(WorkerDict pid=1071018)[0m INFO 08-13 16:17:19 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=8192.
[36m(WorkerDict pid=1071018)[0m WARNING 08-13 16:17:19 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=1071018)[0m local rank 0
[36m(WorkerDict pid=1071018)[0m INFO 08-13 16:17:20 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250813-161720.pkl...
[36m(WorkerDict pid=1071018)[0m INFO 08-13 16:17:20 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20250813-161720.pkl.
