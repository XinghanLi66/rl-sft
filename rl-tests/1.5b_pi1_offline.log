2025-08-06 13:57:47,976	INFO worker.py:1917 -- Started a local Ray instance.
[36m(main_task pid=2294902)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=2294902)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=2294902)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=2294902)[0m                                                  'grad_offload': False,
[36m(main_task pid=2294902)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=2294902)[0m                                                  'param_offload': False,
[36m(main_task pid=2294902)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=2294902)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=2294902)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=2294902)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=2294902)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=2294902)[0m                                            'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=2294902)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=2294902)[0m                                            'total_training_steps': -1,
[36m(main_task pid=2294902)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=2294902)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=2294902)[0m                                  'ppo_max_token_len_per_gpu': 8192,
[36m(main_task pid=2294902)[0m                                  'ppo_micro_batch_size': None,
[36m(main_task pid=2294902)[0m                                  'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=2294902)[0m                                  'ppo_mini_batch_size': 128,
[36m(main_task pid=2294902)[0m                                  'shuffle': False,
[36m(main_task pid=2294902)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=2294902)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=2294902)[0m                                  'use_dynamic_bsz': True,
[36m(main_task pid=2294902)[0m                                  'use_kl_loss': True},
[36m(main_task pid=2294902)[0m                        'hybrid_engine': True,
[36m(main_task pid=2294902)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=2294902)[0m                                  'external_lib': None,
[36m(main_task pid=2294902)[0m                                  'override_config': {},
[36m(main_task pid=2294902)[0m                                  'path': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B',
[36m(main_task pid=2294902)[0m                                  'use_remove_padding': True,
[36m(main_task pid=2294902)[0m                                  'use_think': False},
[36m(main_task pid=2294902)[0m                        'ref': {'fsdp_config': {'param_offload': True,
[36m(main_task pid=2294902)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=2294902)[0m                                'log_prob_max_token_len_per_gpu': 8192,
[36m(main_task pid=2294902)[0m                                'log_prob_micro_batch_size': None,
[36m(main_task pid=2294902)[0m                                'log_prob_micro_batch_size_per_gpu': None,
[36m(main_task pid=2294902)[0m                                'log_prob_use_dynamic_bsz': True,
[36m(main_task pid=2294902)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=2294902)[0m                        'rollout': {'disable_log_stats': True,
[36m(main_task pid=2294902)[0m                                    'do_sample': True,
[36m(main_task pid=2294902)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=2294902)[0m                                    'enable_chunked_prefill': True,
[36m(main_task pid=2294902)[0m                                    'enforce_eager': True,
[36m(main_task pid=2294902)[0m                                    'free_cache_engine': True,
[36m(main_task pid=2294902)[0m                                    'gpu_memory_utilization': 0.6,
[36m(main_task pid=2294902)[0m                                    'ignore_eos': False,
[36m(main_task pid=2294902)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=2294902)[0m                                    'log_prob_max_token_len_per_gpu': 8192,
[36m(main_task pid=2294902)[0m                                    'log_prob_micro_batch_size': None,
[36m(main_task pid=2294902)[0m                                    'log_prob_micro_batch_size_per_gpu': None,
[36m(main_task pid=2294902)[0m                                    'log_prob_use_dynamic_bsz': True,
[36m(main_task pid=2294902)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=2294902)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=2294902)[0m                                    'n': 8,
[36m(main_task pid=2294902)[0m                                    'n_val': 1,
[36m(main_task pid=2294902)[0m                                    'name': 'vllm',
[36m(main_task pid=2294902)[0m                                    'prompt_length': 1024,
[36m(main_task pid=2294902)[0m                                    'response_length': 3072,
[36m(main_task pid=2294902)[0m                                    'temperature': 0.6,
[36m(main_task pid=2294902)[0m                                    'tensor_model_parallel_size': 2,
[36m(main_task pid=2294902)[0m                                    'top_k': -1,
[36m(main_task pid=2294902)[0m                                    'top_p': 1,
[36m(main_task pid=2294902)[0m                                    'val_temperature': 0.6}},
[36m(main_task pid=2294902)[0m  'algorithm': {'adv_estimator': 'grpo',
[36m(main_task pid=2294902)[0m                'gamma': 1.0,
[36m(main_task pid=2294902)[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},
[36m(main_task pid=2294902)[0m                'kl_penalty': 'kl',
[36m(main_task pid=2294902)[0m                'lam': 1.0},
[36m(main_task pid=2294902)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=2294902)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=2294902)[0m             'forward_micro_batch_size': None,
[36m(main_task pid=2294902)[0m             'forward_micro_batch_size_per_gpu': None,
[36m(main_task pid=2294902)[0m             'grad_clip': 1.0,
[36m(main_task pid=2294902)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=2294902)[0m                       'external_lib': None,
[36m(main_task pid=2294902)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=2294902)[0m                                       'optimizer_offload': False,
[36m(main_task pid=2294902)[0m                                       'param_offload': False,
[36m(main_task pid=2294902)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=2294902)[0m                       'override_config': {},
[36m(main_task pid=2294902)[0m                       'path': '~/models/deepseek-llm-7b-chat',
[36m(main_task pid=2294902)[0m                       'tokenizer_path': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B',
[36m(main_task pid=2294902)[0m                       'use_remove_padding': False},
[36m(main_task pid=2294902)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=2294902)[0m                       'lr_warmup_steps_ratio': 0.0,
[36m(main_task pid=2294902)[0m                       'min_lr_ratio': None,
[36m(main_task pid=2294902)[0m                       'total_training_steps': -1,
[36m(main_task pid=2294902)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=2294902)[0m No module named 'vllm._version'
[36m(main_task pid=2294902)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=2295994)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=2295994)[0m No module named 'vllm._version'
[36m(pid=2295994)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(pid=2296417)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=2296417)[0m No module named 'vllm._version'
[36m(pid=2296417)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=2295994)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(main_task pid=2294902)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=2294902)[0m             'ppo_epochs': 1,
[36m(main_task pid=2294902)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=2294902)[0m             'ppo_micro_batch_size': None,
[36m(main_task pid=2294902)[0m             'ppo_micro_batch_size_per_gpu': None,
[36m(main_task pid=2294902)[0m             'ppo_mini_batch_size': 128,
[36m(main_task pid=2294902)[0m             'shuffle': False,
[36m(main_task pid=2294902)[0m             'strategy': 'fsdp',
[36m(main_task pid=2294902)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=2294902)[0m             'use_dynamic_bsz': True},
[36m(main_task pid=2294902)[0m  'data': {'max_prompt_length': 1024,
[36m(main_task pid=2294902)[0m           'max_response_length': 3072,
[36m(main_task pid=2294902)[0m           'prompt_key': 'prompt',
[36m(main_task pid=2294902)[0m           'return_raw_chat': False,
[36m(main_task pid=2294902)[0m           'return_raw_input_ids': False,
[36m(main_task pid=2294902)[0m           'shuffle': True,
[36m(main_task pid=2294902)[0m           'tokenizer': None,
[36m(main_task pid=2294902)[0m           'train_batch_size': 128,
[36m(main_task pid=2294902)[0m           'train_files': 'data/train/one_shot_rlvr/pi1_r128.parquet',
[36m(main_task pid=2294902)[0m           'val_batch_size': 530,
[36m(main_task pid=2294902)[0m           'val_files': 'data/test/math_minerva_aime25x8.parquet'},
[36m(main_task pid=2294902)[0m  'reward_model': {'enable': False,
[36m(main_task pid=2294902)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=2294902)[0m                   'max_length': None,
[36m(main_task pid=2294902)[0m                   'micro_batch_size': None,
[36m(main_task pid=2294902)[0m                   'micro_batch_size_per_gpu': None,
[36m(main_task pid=2294902)[0m                   'model': {'external_lib': None,
[36m(main_task pid=2294902)[0m                             'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=2294902)[0m                                             'min_num_params': 0,
[36m(main_task pid=2294902)[0m                                             'param_offload': False},
[36m(main_task pid=2294902)[0m                             'input_tokenizer': '/homes/gws/lxh22/models/Qwen2.5-Math-1.5B',
[36m(main_task pid=2294902)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=2294902)[0m                             'use_remove_padding': False},
[36m(main_task pid=2294902)[0m                   'reward_manager': 'naive',
[36m(main_task pid=2294902)[0m                   'strategy': 'fsdp',
[36m(main_task pid=2294902)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=2294902)[0m                   'use_dynamic_bsz': True},
[36m(main_task pid=2294902)[0m  'trainer': {'checkpoints_dir': '/local1/lxh/save',
[36m(main_task pid=2294902)[0m              'critic_warmup': 0,
[36m(main_task pid=2294902)[0m              'default_hdfs_dir': None,
[36m(main_task pid=2294902)[0m              'default_local_dir': '/local1/lxh/save/verl_few_shot/Qwen2.5-Math-1.5B-pi1_offline_0805',
[36m(main_task pid=2294902)[0m              'del_local_ckpt_after_load': False,
[36m(main_task pid=2294902)[0m              'experiment_name': 'Qwen2.5-Math-1.5B-pi1_offline_0805',
[36m(main_task pid=2294902)[0m              'logger': ['console', 'wandb'],
[36m(main_task pid=2294902)[0m              'n_gpus_per_node': 4,
[36m(main_task pid=2294902)[0m              'nnodes': 1,
[36m(main_task pid=2294902)[0m              'project_name': 'verl_few_shot',
[36m(main_task pid=2294902)[0m              'remove_previous_ckpt_in_save': False,
[36m(main_task pid=2294902)[0m              'resume_from_path': False,
[36m(main_task pid=2294902)[0m              'resume_mode': 'auto',
[36m(main_task pid=2294902)[0m              'save_freq': 20,
[36m(main_task pid=2294902)[0m              'test_freq': 20,
[36m(main_task pid=2294902)[0m              'total_epochs': 200,
[36m(main_task pid=2294902)[0m              'total_training_steps': None,
[36m(main_task pid=2294902)[0m              'val_before_train': True,
[36m(main_task pid=2294902)[0m              'val_generations_to_log_to_wandb': 0}}
[36m(main_task pid=2294902)[0m 
[36m(main_task pid=2294902)[0m Qwen or LLAMA---------------------------------
[36m(main_task pid=2294902)[0m 
[36m(main_task pid=2294902)[0m [validate_config] All configuration checks passed successfully!
[36m(main_task pid=2294902)[0m original dataset len: 128
[36m(main_task pid=2294902)[0m filter dataset len: 128
[36m(main_task pid=2294902)[0m Train_dataset size 128
[36m(main_task pid=2294902)[0m original dataset len: 1012
[36m(main_task pid=2294902)[0m filter dataset len: 1012
[36m(main_task pid=2294902)[0m Size of train dataloader: 1
[36m(main_task pid=2294902)[0m Size of val dataloader: 1
[36m(main_task pid=2294902)[0m Total training steps: 200
[36m(WorkerDict pid=2295994)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2295994)[0m   "architectures": [
[36m(WorkerDict pid=2295994)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2295994)[0m   ],
[36m(WorkerDict pid=2295994)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2295994)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=2295994)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2295994)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=2295994)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2295994)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=2295994)[0m   "max_position_embeddings": 4096,
[36m(WorkerDict pid=2295994)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=2295994)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2295994)[0m   "num_attention_heads": 12,
[36m(WorkerDict pid=2295994)[0m   "num_hidden_layers": 28,
[36m(WorkerDict pid=2295994)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=2295994)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2295994)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2295994)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2295994)[0m   "rope_theta": 10000,
[36m(WorkerDict pid=2295994)[0m   "sliding_window": 4096,
[36m(WorkerDict pid=2295994)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2295994)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2295994)[0m   "transformers_version": "4.51.3",
[36m(WorkerDict pid=2295994)[0m   "use_cache": true,
[36m(WorkerDict pid=2295994)[0m   "use_mrope": false,
[36m(WorkerDict pid=2295994)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2295994)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2295994)[0m }
[36m(WorkerDict pid=2295994)[0m 
[36m(WorkerDict pid=2295994)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=2295994)[0m Qwen2ForCausalLM contains 1.54B parameters
[36m(WorkerDict pid=2295994)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f871857e050>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f871857df30>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=2295994)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2295994)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2295994)[0m   "architectures": [
[36m(WorkerDict pid=2295994)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2295994)[0m   ],
[36m(WorkerDict pid=2295994)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2295994)[0m   "eos_token_id": 151643,
[36m(WorkerDict pid=2295994)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2295994)[0m   "hidden_size": 1536,
[36m(WorkerDict pid=2295994)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2295994)[0m   "intermediate_size": 8960,
[36m(WorkerDict pid=2295994)[0m   "max_position_embeddings": 4096,
[36m(WorkerDict pid=2295994)[0m   "max_window_layers": 21,
[36m(WorkerDict pid=2295994)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2295994)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(pid=2296418)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=2296418)[0m No module named 'vllm._version'[32m [repeated 2x across cluster][0m
[36m(pid=2296418)[0m   from vllm.version import __version__ as VLLM_VERSION[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=2296418)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2296417)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=2296417)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=2296418)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2296417)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=2296416)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2296416)[0m   warnings.warn(
[36m(WorkerDict pid=2295994)[0m /homes/gws/lxh22/miniconda3/envs/rlvr_train/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=2295994)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2295994)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")[32m [repeated 3x across cluster][0m
[36m(main_task pid=2294902)[0m wandb: Currently logged in as: lixinghan2013 (coder66-RL-lab) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(main_task pid=2294902)[0m wandb: Tracking run with wandb version 0.20.1
[36m(main_task pid=2294902)[0m wandb: Run data is saved locally in /homes/gws/lxh22/rl-sft/rl-tests/wandb/run-20250806_135908-exmzsvch
[36m(main_task pid=2294902)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(main_task pid=2294902)[0m wandb: Syncing run Qwen2.5-Math-1.5B-pi1_offline_0805
[36m(main_task pid=2294902)[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/coder66-RL-lab/verl_few_shot
[36m(main_task pid=2294902)[0m wandb: üöÄ View run at https://wandb.ai/coder66-RL-lab/verl_few_shot/runs/exmzsvch
[36m(WorkerDict pid=2295994)[0m /homes/gws/lxh22/rl-sft/rl-tests/verl/utils/checkpoint/fsdp_checkpoint_manager.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[36m(WorkerDict pid=2295994)[0m   model_state_dict = torch.load(local_model_path)
[36m(WorkerDict pid=2295994)[0m   optimizer_state_dict = torch.load(local_optim_path)
[36m(WorkerDict pid=2295994)[0m   extra_state_dict = torch.load(local_extra_state_path)
[36m(main_task pid=2294902)[0m   self.train_dataloader = torch.load(dataloader_local_path)
